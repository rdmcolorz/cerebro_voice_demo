{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from IPython import display\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# NOTES\n",
    "NOTES = \"28x28\"\n",
    "\n",
    "# VARS\n",
    "INPUT_CSV = \"taylor4_12_04\"\n",
    "CATEGORY = [\"no_voice\"]\n",
    "LABELS = [\"lights-on\", \"turn-off\"]\n",
    "CHANNELS = [1,2,3,4]\n",
    "NUMS = ''.join([str(x) for x in CHANNELS])\n",
    "MONTHS = [12]\n",
    "DAYS = [4]\n",
    "\n",
    "target_label = \"Label\"\n",
    "id_label = \"fname\"\n",
    "OUTSTR = \"A{:.4f}_Type{}_UDR{}_THRESH{:5.3f}_DROP{}_LR{}_S{}_B{}{}.csv\"\n",
    "IMG_EXT = \".png\"\n",
    "VERBOSE = True\n",
    "DISPLAY = True\n",
    "TEST = False\n",
    "MFCC = False\n",
    "TPU = False\n",
    "RESIZE = True\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "TARGET_WIDTH = 28 if RESIZE else INPUT_WIDTH\n",
    "TARGET_HEIGHT = 28 if RESIZE else INPUT_HEIGHT\n",
    "DECAY_RATE = 0.9\n",
    "IMG_CHANNELS = 3\n",
    "DROPOUT = 0.4\n",
    "TYPE = \"CNN\"\n",
    "DEFAULT_BS = 128 # default batch size\n",
    "UNK_DROP_RATE = 1.0 # drop 100% of unknown categories\n",
    "\n",
    "if TEST:\n",
    "    LEARNING_STEPS = 100\n",
    "    SPP = 4\n",
    "    LEARNING_RATE = .05\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSITY = 1000\n",
    "    TEST_SIZE = 1000\n",
    "    SHUFFLE_SIZE = 64\n",
    "else:\n",
    "    LEARNING_STEPS = 5000\n",
    "    SPP = 200\n",
    "    LEARNING_RATE = .025\n",
    "    BATCH_SIZE = 64\n",
    "    VERBOSITY = 1000\n",
    "    SHUFFLE_SIZE = 256\n",
    "\n",
    "def curr_time():\n",
    "    return datetime.now() - timedelta(hours=7) # offset from UTC to PST\n",
    "\n",
    "ROOT = os.getcwd() + \"/\"\n",
    "if CATEGORY[0] == \"no_voice\":\n",
    "    RUN_ROOT = ROOT+\"models/\"+\"NONVOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "else:\n",
    "    RUN_ROOT = ROOT + \"models/\" + \"VOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "RUN_ROOT_LOG = RUN_ROOT + \"logs/\"\n",
    "\n",
    "# PATHS\n",
    "paths = {\n",
    "    \"Training\":ROOT + \"train_csv/\" + INPUT_CSV + \".csv\",\n",
    "    \"Model\":RUN_ROOT+\"model_dir_{}/\".format(NUMS),\n",
    "    \"Logs\":RUN_ROOT_LOG+\"{}_{}/\".format(NUMS, datetime.strftime(curr_time(), \"%b%d%Y_%H%M%S\"))\n",
    "}\n",
    "paths[\"Log\"] = paths[\"Logs\"] + \"log.txt\"\n",
    "if not os.path.isdir(RUN_ROOT):\n",
    "    os.mkdir(RUN_ROOT)\n",
    "if not os.path.isdir(RUN_ROOT_LOG):\n",
    "    os.mkdir(RUN_ROOT_LOG)\n",
    "if not os.path.isdir(paths[\"Logs\"]):\n",
    "    os.mkdir(paths[\"Logs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_header(s):\n",
    "    return (\"#\" * 42) + (\"\\n{:^42}\\n\".format(s)) + (\"#\" * 42)\n",
    "    \n",
    "def print_and_log(s):\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(str(s))\n",
    "        log.write(\"\\n\")\n",
    "    print(s)\n",
    "        \n",
    "def print_and_log_header(s):\n",
    "    h = make_header(str(s))\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(h)\n",
    "        log.write(\"\\n\")\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sec_to_str(secs):\n",
    "    ms = secs - int(secs)\n",
    "    days = int(secs // (24 * 3600))\n",
    "    hours = int((secs % ((24 * 3600))) // 3600)\n",
    "    minutes = int((secs % 3600) // 60)\n",
    "    seconds = int(secs % 60)\n",
    "    return \"{:02}:{:02}:{:02}:{:02}.{}\".format(days, hours, minutes, seconds, \"{:.3}\".format(ms)[2:])\n",
    "\n",
    "def timer(f, *args):\n",
    "    print_and_log(\"Start: {}\".format(curr_time()))\n",
    "    start = time.time()\n",
    "    result = f(*args)\n",
    "    end = time.time()\n",
    "    print_and_log(\"End: {}\".format(curr_time()))\n",
    "    print_and_log(\"Finished in {}\".format(sec_to_str(end - start)))\n",
    "    return result\n",
    "\n",
    "def preprocess(samples, sample_rate):\n",
    "    padded = np.zeros(sample_rate)\n",
    "    samples = samples[:sample_rate]\n",
    "    padded[:samples.shape[0]] = samples\n",
    "    return padded\n",
    "\n",
    "def select_labels(df, allowed):\n",
    "    return df[df['Label'].isin(allowed)]\n",
    "    \n",
    "def select_categories(df, allowed):\n",
    "    return df[df['Category'].isin(allowed)]\n",
    "\n",
    "def select_channels(df, allowed):\n",
    "    labels = []\n",
    "    for i in range(1, 9):\n",
    "        if i not in allowed:\n",
    "            labels.append(\"Path{}\".format(i))\n",
    "    return df.drop(labels, axis=1)\n",
    "\n",
    "def select_days(df, allowed):\n",
    "    return df[df['Day'].isin(allowed)]\n",
    "\n",
    "def select_months(df, allowed):\n",
    "    return df[df['Month'].isin(allowed)]\n",
    "\n",
    "def select_sets(df, allowed):\n",
    "    return df[df['Set'].isin(allowed)]\n",
    "\n",
    "def remove_voice(df):\n",
    "    return df.drop([\"Path4\"], axis=1)\n",
    "\n",
    "def str_to_l(x):\n",
    "    return [int(n) for n in x if n <= '9' and n >= '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def _parse_function(label, *filenames):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % VERBOSITY == 0:\n",
    "        print_and_log(\"\\tProcessed {}th image\".format(count))\n",
    "    expected_shape = tf.constant([1, INPUT_HEIGHT, INPUT_WIDTH, IMG_CHANNELS])\n",
    "    image = None\n",
    "    for filename in filenames:\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_image(image_string, channels=IMG_CHANNELS)\n",
    "        image_decoded = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "        image_decoded = tf.reshape(image_decoded, expected_shape)\n",
    "        image_decoded = tf.image.rgb_to_grayscale(image_decoded)\n",
    "        if RESIZE:\n",
    "            image_decoded = tf.image.resize_bicubic(image_decoded, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "        if image is not None:\n",
    "            image = tf.concat([image, image_decoded], 3)\n",
    "        else:\n",
    "            image = image_decoded\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    input_layer = tf.reshape(features, [-1, TARGET_HEIGHT, TARGET_WIDTH, len(CHANNELS)])\n",
    "    pool = input_layer\n",
    "\n",
    "    for num_filters in [32, 64]:\n",
    "        conv = tf.layers.conv2d(\n",
    "            inputs=pool,\n",
    "            filters=num_filters,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "        pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool = tf.layers.flatten(pool)\n",
    "    dense = tf.layers.dense(inputs=pool, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=DROPOUT, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_labels)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if not TPU:\n",
    "        tf.summary.histogram(\"predictions\", predictions[\"probabilities\"])\n",
    "        tf.summary.histogram(\"classes\", predictions[\"classes\"])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE, tf.train.get_global_step(), SPP, DECAY_RATE, staircase=True)\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    predictions[\"loss\"] = loss\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        if TPU:\n",
    "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_input_fn(dataset, batch_size, num_epochs=None):\n",
    "    def _input_fn(num_epochs=None, shuffle=True):\n",
    "        ds = dataset.batch(batch_size).repeat(num_epochs)\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(SHUFFLE_SIZE)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn\n",
    "\n",
    "def create_predict_input_fn(dataset, batch_size):\n",
    "    def _input_fn():\n",
    "        ds = dataset.batch(batch_size)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn\n",
    "\n",
    "def train_helper(steps_per_period):\n",
    "    classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=steps_per_period)\n",
    "    training_stats = classifier.evaluate(input_fn=training_eval_input_fn)\n",
    "    validation_stats = classifier.evaluate(input_fn=validation_eval_input_fn)\n",
    "    t_ll = training_stats[\"loss\"]\n",
    "    t_acc = 100 * training_stats[\"accuracy\"]\n",
    "    v_ll = validation_stats[\"loss\"]\n",
    "    v_acc = 100 * validation_stats[\"accuracy\"]\n",
    "    return classifier, t_ll, v_ll, t_acc, v_acc\n",
    "\n",
    "def train():\n",
    "    periods = LEARNING_STEPS // SPP\n",
    "    steps_per_period = LEARNING_STEPS // periods\n",
    "    t_accs, v_accs = [], []\n",
    "    t_lls, v_lls = [], []\n",
    "    print_and_log(\"Training model...\\nMetrics:\")\n",
    "    print_and_log(\"\\tPERIOD\\tRATE\\tTYPE\\tTRAIN.\\tVALID.\\tTIME\")\n",
    "    for period in range(periods):\n",
    "        lr = LEARNING_RATE * (DECAY_RATE ** ((period * SPP) / SPP))\n",
    "        classifier, t_ll, v_ll, t_acc, v_acc = train_helper(steps_per_period)\n",
    "        print_and_log(\"\\t{}\\t{:.5f}\\tLgLs\\t{:.2f}\\t{:.2f}\\t{}\".format(period, lr, t_ll, v_ll, curr_time()))\n",
    "        print_and_log(\"\\t\\t\\tAcc.\\t{:.2f}%\\t{:.2f}%\\n\".format(t_acc, v_acc))\n",
    "        t_lls.append(t_ll)\n",
    "        v_lls.append(v_ll)\n",
    "        t_accs.append(t_acc)\n",
    "        v_accs.append(v_acc)\n",
    "    v_accuracy = v_accs[-1]\n",
    "    return classifier, v_accuracy, t_lls, v_lls, t_accs, v_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(paths[\"Log\"], 'w') as log:\n",
    "    log.write(make_header(\"Starting Script\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create variables for the paths\n",
    "train_csv = paths[\"Training\"]\n",
    "\n",
    "# Store the labels to train\n",
    "all_labels = LABELS\n",
    "labels = LABELS\n",
    "num_labels = len(labels)# - 1\n",
    "labels = {x[1]:x[0] for x in enumerate(labels)}\n",
    "reverse_lookup = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "           MAKING TRAINING DATA           \n",
      "##########################################\n",
      "##########################################\n",
      "                TRAIN DATA                \n",
      "##########################################\n",
      "          Day   Month        Label  SequenceNumber  Path9  Path10  Path11  \\\n",
      "count  2038.0  2038.0  2038.000000     2038.000000    0.0     0.0     0.0   \n",
      "mean      4.0    12.0     0.500000      509.000000    NaN     NaN     NaN   \n",
      "std       0.0     0.0     0.500123      294.232016    NaN     NaN     NaN   \n",
      "min       4.0    12.0     0.000000        0.000000    NaN     NaN     NaN   \n",
      "25%       4.0    12.0     0.000000      254.250000    NaN     NaN     NaN   \n",
      "50%       4.0    12.0     0.500000      509.000000    NaN     NaN     NaN   \n",
      "75%       4.0    12.0     1.000000      763.750000    NaN     NaN     NaN   \n",
      "max       4.0    12.0     1.000000     1018.000000    NaN     NaN     NaN   \n",
      "\n",
      "       Path12  Path13  Path14  Path15  Path16  Path17  Path18  Path19  Path20  \\\n",
      "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "       Path21  Path22  \n",
      "count     0.0     0.0  \n",
      "mean      NaN     NaN  \n",
      "std       NaN     NaN  \n",
      "min       NaN     NaN  \n",
      "25%       NaN     NaN  \n",
      "50%       NaN     NaN  \n",
      "75%       NaN     NaN  \n",
      "max       NaN     NaN  \n",
      "   Category  Day  Month  Label  SequenceNumber         Set  \\\n",
      "0  no_voice    4     12      0             991    Training   \n",
      "1  no_voice    4     12      0             943    Training   \n",
      "2  no_voice    4     12      1             171    Training   \n",
      "3  no_voice    4     12      0              63    Training   \n",
      "4  no_voice    4     12      1             104    Training   \n",
      "5  no_voice    4     12      0             368  Validation   \n",
      "6  no_voice    4     12      1             183    Training   \n",
      "7  no_voice    4     12      0             264    Training   \n",
      "8  no_voice    4     12      1             418  Validation   \n",
      "9  no_voice    4     12      0             130    Training   \n",
      "\n",
      "                                               Path1  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "\n",
      "                                               Path2  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "\n",
      "                                               Path3  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor4...   \n",
      "\n",
      "                                               Path4   ...    Path13  Path14  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor4...   ...       NaN     NaN   \n",
      "\n",
      "   Path15  Path16  Path17  Path18  Path19  Path20  Path21  Path22  \n",
      "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "5     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "6     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "7     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "9     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[10 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make the training data\n",
    "print_and_log_header(\"MAKING TRAINING DATA\")\n",
    "train_data = pd.read_csv(train_csv)\n",
    "\n",
    "# Filter the training data\n",
    "train_data = select_categories(train_data, CATEGORY)\n",
    "train_data = select_channels(train_data, CHANNELS)\n",
    "train_data = select_labels(train_data, all_labels)\n",
    "train_data = select_months(train_data, MONTHS)\n",
    "train_data = select_days(train_data, DAYS)\n",
    "# train_data = remove_voice(train_data)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "train_data[\"Label\"] = train_data[\"Label\"].map(labels)\n",
    "\n",
    "if VERBOSE:\n",
    "    print_and_log_header(\"TRAIN DATA\")\n",
    "    print_and_log(train_data.describe())\n",
    "    print_and_log(train_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab subset of the data for testing purposes\n",
    "if TEST:\n",
    "    train_data = train_data[:TEST_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 80/10/10 training/validation/test split\n",
    "validation_data = select_sets(train_data, [\"Validation\"])\n",
    "test_data = select_sets(train_data, [\"Testing\"])\n",
    "train_data = select_sets(train_data, [\"Training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                   IDS                    \n",
      "##########################################\n",
      "count                                                   202\n",
      "unique                                                  202\n",
      "top       /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "freq                                                      1\n",
      "Name: Path1, dtype: object\n",
      "45     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "51     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "57     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "63     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "66     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "78     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "84     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "87     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "96     /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "121    /Users/kyy/cerebro_train/images_scaled/taylor4...\n",
      "Name: Path1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ids = test_data[\"Path{}\".format(CHANNELS[0])] # store the png filenames for output\n",
    "if VERBOSE:\n",
    "    print_and_log_header(\"IDS\")\n",
    "    print_and_log(ids.describe())\n",
    "    print_and_log(ids.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate Labels\n",
    "train_labels = train_data.pop(target_label)\n",
    "validation_labels = validation_data.pop(target_label)\n",
    "test_labels = test_data.pop(target_label)\n",
    "img_paths = [\"Path{}\".format(channel) for channel in CHANNELS]\n",
    "train_data = train_data[img_paths]\n",
    "validation_data = validation_data[img_paths]\n",
    "test_data = test_data[img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "          Parsing Training Data           \n",
      "##########################################\n",
      "Start: 2018-12-04 16:04:58.859333\n",
      "End: 2018-12-04 16:04:59.176004\n",
      "Finished in 00:00:00:00.316\n",
      "##########################################\n",
      "         Parsing Validation Data          \n",
      "##########################################\n",
      "Start: 2018-12-04 16:04:59.183033\n",
      "End: 2018-12-04 16:04:59.543907\n",
      "Finished in 00:00:00:00.36\n",
      "##########################################\n",
      "           Parsing Testing Data           \n",
      "##########################################\n",
      "Start: 2018-12-04 16:04:59.550825\n",
      "End: 2018-12-04 16:04:59.838097\n",
      "Finished in 00:00:00:00.287\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Vectors of filenames.\n",
    "t_f, v_f, s_f = [], [], []\n",
    "for i in range(1, 1 + len(CHANNELS)):\n",
    "    channel = CHANNELS[i-1]\n",
    "    l = \"Path{}\".format(channel)\n",
    "    t_f.append(tf.constant(train_data[l]))\n",
    "    v_f.append(tf.constant(validation_data[l]))\n",
    "    s_f.append(tf.constant(test_data[l]))\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i]\n",
    "# Vectors of labels\n",
    "train_labels = tf.constant(train_labels)\n",
    "validation_labels = tf.constant(validation_labels)\n",
    "test_labels = tf.constant(test_labels)\n",
    "\n",
    "# Make datasets from filenames and labels\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_labels, *t_f))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((validation_labels, *v_f))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_labels, *s_f))\n",
    "print_and_log_header(\"Parsing Training Data\")\n",
    "train_data = timer(lambda: train_data.map(_parse_function))\n",
    "print_and_log_header(\"Parsing Validation Data\")\n",
    "validation_data = timer(lambda: validation_data.map(_parse_function))\n",
    "print_and_log_header(\"Parsing Testing Data\")\n",
    "test_data = timer(lambda: test_data.map(_parse_function))\n",
    "print_and_log(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 TRAINING                 \n",
      "##########################################\n",
      "<MapDataset shapes: ((1, 28, 28, 4), ()), types: (tf.float32, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
      "##########################################\n",
      "                VALIDATION                \n",
      "##########################################\n",
      "<MapDataset shapes: ((1, 28, 28, 4), ()), types: (tf.float32, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
      "##########################################\n",
      "                 TESTING                  \n",
      "##########################################\n",
      "<MapDataset shapes: ((1, 28, 28, 4), ()), types: (tf.float32, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print_and_log_header(\"TRAINING\")\n",
    "print_and_log(train_data)\n",
    "print_and_log(type(train_data))\n",
    "print_and_log_header(\"VALIDATION\")\n",
    "print_and_log(validation_data)\n",
    "print_and_log(type(validation_data))\n",
    "print_and_log_header(\"TESTING\")\n",
    "print_and_log(test_data)\n",
    "print_and_log(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "if TPU:\n",
    "    classifier = tf.contrib.tpu.TPUEstimator(\n",
    "        model_fn=model_fn,\n",
    "        model_dir=paths[\"Model\"],\n",
    "        config=tf.contrib.tpu.RunConfig(),\n",
    "        use_tpu=TPU)\n",
    "else:\n",
    "    classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=paths[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the input functions.\n",
    "training_eval_input_fn = create_predict_input_fn(train_data, DEFAULT_BS)\n",
    "validation_eval_input_fn = create_predict_input_fn(validation_data, DEFAULT_BS)\n",
    "test_eval_input_fn = create_predict_input_fn(test_data, DEFAULT_BS)\n",
    "train_input_fn = create_training_input_fn(train_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-12-04 16:04:59.873881\n",
      "Training model...\n",
      "Metrics:\n",
      "\tPERIOD\tRATE\tTYPE\tTRAIN.\tVALID.\tTIME\n",
      "\t0\t0.02500\tLgLs\t0.69\t0.69\t2018-12-04 16:05:57.000178\n",
      "\t\t\tAcc.\t50.00%\t50.00%\n",
      "\n",
      "\t1\t0.02250\tLgLs\t0.69\t0.69\t2018-12-04 16:06:51.575274\n",
      "\t\t\tAcc.\t50.98%\t50.98%\n",
      "\n",
      "\t2\t0.02025\tLgLs\t0.69\t0.69\t2018-12-04 16:07:44.667756\n",
      "\t\t\tAcc.\t55.76%\t52.45%\n",
      "\n",
      "\t3\t0.01823\tLgLs\t0.69\t0.69\t2018-12-04 16:08:34.904513\n",
      "\t\t\tAcc.\t56.56%\t50.49%\n",
      "\n",
      "\t4\t0.01640\tLgLs\t0.69\t0.69\t2018-12-04 16:09:24.712101\n",
      "\t\t\tAcc.\t56.00%\t51.96%\n",
      "\n",
      "\t5\t0.01476\tLgLs\t0.69\t0.69\t2018-12-04 16:10:13.696098\n",
      "\t\t\tAcc.\t56.25%\t51.47%\n",
      "\n",
      "\t6\t0.01329\tLgLs\t0.69\t0.69\t2018-12-04 16:11:02.831867\n",
      "\t\t\tAcc.\t56.43%\t50.98%\n",
      "\n",
      "\t7\t0.01196\tLgLs\t0.69\t0.69\t2018-12-04 16:11:51.616016\n",
      "\t\t\tAcc.\t56.13%\t51.47%\n",
      "\n",
      "\t8\t0.01076\tLgLs\t0.69\t0.69\t2018-12-04 16:12:41.443904\n",
      "\t\t\tAcc.\t55.70%\t51.96%\n",
      "\n",
      "\t9\t0.00969\tLgLs\t0.69\t0.69\t2018-12-04 16:13:30.266496\n",
      "\t\t\tAcc.\t54.90%\t51.96%\n",
      "\n",
      "\t10\t0.00872\tLgLs\t0.69\t0.69\t2018-12-04 16:14:19.406979\n",
      "\t\t\tAcc.\t56.00%\t51.47%\n",
      "\n",
      "\t11\t0.00785\tLgLs\t0.69\t0.69\t2018-12-04 16:15:08.734114\n",
      "\t\t\tAcc.\t55.27%\t52.45%\n",
      "\n",
      "\t12\t0.00706\tLgLs\t0.69\t0.69\t2018-12-04 16:15:58.194527\n",
      "\t\t\tAcc.\t56.13%\t49.51%\n",
      "\n",
      "\t13\t0.00635\tLgLs\t0.68\t0.69\t2018-12-04 16:16:47.400656\n",
      "\t\t\tAcc.\t56.43%\t50.98%\n",
      "\n",
      "\t14\t0.00572\tLgLs\t0.69\t0.69\t2018-12-04 16:17:37.069894\n",
      "\t\t\tAcc.\t56.92%\t53.92%\n",
      "\n",
      "\t15\t0.00515\tLgLs\t0.68\t0.69\t2018-12-04 16:18:26.308753\n",
      "\t\t\tAcc.\t56.07%\t51.96%\n",
      "\n",
      "\t16\t0.00463\tLgLs\t0.68\t0.69\t2018-12-04 16:19:15.395662\n",
      "\t\t\tAcc.\t56.37%\t50.98%\n",
      "\n",
      "\t17\t0.00417\tLgLs\t0.68\t0.69\t2018-12-04 16:20:04.908512\n",
      "\t\t\tAcc.\t55.88%\t51.96%\n",
      "\n",
      "\t18\t0.00375\tLgLs\t0.68\t0.69\t2018-12-04 16:20:54.622068\n",
      "\t\t\tAcc.\t56.37%\t51.96%\n",
      "\n",
      "\t19\t0.00338\tLgLs\t0.68\t0.69\t2018-12-04 16:21:44.020375\n",
      "\t\t\tAcc.\t56.56%\t51.96%\n",
      "\n",
      "\t20\t0.00304\tLgLs\t0.68\t0.69\t2018-12-04 16:22:33.421715\n",
      "\t\t\tAcc.\t56.19%\t53.43%\n",
      "\n",
      "\t21\t0.00274\tLgLs\t0.68\t0.69\t2018-12-04 16:23:23.679127\n",
      "\t\t\tAcc.\t55.82%\t52.45%\n",
      "\n",
      "\t22\t0.00246\tLgLs\t0.68\t0.69\t2018-12-04 16:24:13.123709\n",
      "\t\t\tAcc.\t57.54%\t54.41%\n",
      "\n",
      "\t23\t0.00222\tLgLs\t0.68\t0.69\t2018-12-04 16:25:03.112615\n",
      "\t\t\tAcc.\t56.00%\t51.47%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "classifier, accuracy, t_lls, v_lls, t_accs, v_accs = timer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Final accuracy (on validation data): {:.4f}%\".format(accuracy))\n",
    "\n",
    "if DISPLAY:\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(t_lls, label=\"training\")\n",
    "    plt.plot(v_lls, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(paths[\"Logs\"] + \"loss.png\")\n",
    "\n",
    "    # Output a graph of accuracy over periods.\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"Accuracy vs. Periods\")\n",
    "    plt.plot(t_accs, label=\"training\")\n",
    "    plt.plot(v_accs, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(paths[\"Logs\"] + \"accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_stats = classifier.evaluate(input_fn=test_eval_input_fn)\n",
    "t_ll = test_stats[\"loss\"]\n",
    "t_acc = 100 * test_stats[\"accuracy\"]\n",
    "print_and_log_header(\"TESTING\")\n",
    "print_and_log(\"\\tLog Loss: {:.2f}\".format(t_ll))\n",
    "print_and_log(\"\\tAccuracy: {:.2f}%\".format(t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
