{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kyy/cerebro_train/NONVOCAL_RUNS_YN_12_06/models/NONVOCAL_RUNS_YN_12_06_60%/model_dir_1234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from IPython import display\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# NOTES\n",
    "NOTES = \"28x28\"\n",
    "\n",
    "# VARS\n",
    "target_label = \"Label\"\n",
    "id_label = \"fname\"\n",
    "VERBOSE = True\n",
    "DISPLAY = True\n",
    "TEST = False\n",
    "TPU = False\n",
    "RESIZE = True\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "TARGET_WIDTH = 28 if RESIZE else INPUT_WIDTH\n",
    "TARGET_HEIGHT = 28 if RESIZE else INPUT_HEIGHT\n",
    "DECAY_RATE = 0.9\n",
    "IMG_CHANNELS = 3\n",
    "DROPOUT = 0.4\n",
    "TYPE = \"CNN\"\n",
    "DEFAULT_BS = 128 # default batch size\n",
    "UNK_DROP_RATE = 1.0 # drop 100% of unknown categories\n",
    "OUTLIER_PERCENTAGE = 0.1\n",
    "\n",
    "CATEGORY = [\"no_voice\"]\n",
    "LABELS = [\"lights-on\", \"turn-off\"]\n",
    "CHANNELS = [1, 2, 3, 4]\n",
    "NUMS = ''.join([str(x) for x in CHANNELS])\n",
    "MONTHS = [12]\n",
    "DAYS = [6]\n",
    "\n",
    "if TEST:\n",
    "    LEARNING_STEPS = 100\n",
    "    SPP = 4\n",
    "    LEARNING_RATE = .05\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSITY = 1000\n",
    "    TEST_SIZE = 1000\n",
    "    SHUFFLE_SIZE = 64\n",
    "else:\n",
    "    LEARNING_STEPS = 5000\n",
    "    SPP = 200\n",
    "    LEARNING_RATE = .025\n",
    "    BATCH_SIZE = 64\n",
    "    VERBOSITY = 1000\n",
    "    SHUFFLE_SIZE = 256\n",
    "\n",
    "def curr_time():\n",
    "    return datetime.now() - timedelta(hours=7) # offset from UTC to PST\n",
    "\n",
    "ROOT = os.getcwd() + \"/\"\n",
    "if CATEGORY[0] == \"no_voice\":\n",
    "    RUN_ROOT = ROOT+\"NONVOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "else:\n",
    "    RUN_ROOT = ROOT+\"VOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "RUN_ROOT_LOG = RUN_ROOT+\"logs/\"\n",
    "\n",
    "# PATHS\n",
    "paths = {\n",
    "    \"Training\":ROOT+\"train_csv/taylor_12_06_01.csv\",\n",
    "    \"Model\":RUN_ROOT+\"models/NONVOCAL_RUNS_YN_12_06_60%/model_dir_1234\",# model_dir_{}/.format(NUMS),\n",
    "    \"Logs\":RUN_ROOT_LOG+\"{}_{}/\".format(NUMS, datetime.strftime(curr_time(), \"%b%d%Y_%H%M%S\"))\n",
    "}\n",
    "paths[\"Log\"] = paths[\"Logs\"] + \"log.txt\"\n",
    "if not os.path.isdir(RUN_ROOT):\n",
    "    os.mkdir(RUN_ROOT)\n",
    "if not os.path.isdir(RUN_ROOT_LOG):\n",
    "    os.mkdir(RUN_ROOT_LOG)\n",
    "if not os.path.isdir(paths[\"Logs\"]):\n",
    "    os.mkdir(paths[\"Logs\"])\n",
    "    \n",
    "print(paths['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_header(s):\n",
    "    return (\"#\" * 42) + (\"\\n{:^42}\\n\".format(s)) + (\"#\" * 42)\n",
    "    \n",
    "def print_and_log(s):\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(str(s))\n",
    "        log.write(\"\\n\")\n",
    "    print(s)\n",
    "        \n",
    "def print_and_log_header(s):\n",
    "    h = make_header(str(s))\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(h)\n",
    "        log.write(\"\\n\")\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sec_to_str(secs):\n",
    "    ms = secs - int(secs)\n",
    "    days = int(secs // (24 * 3600))\n",
    "    hours = int((secs % ((24 * 3600))) // 3600)\n",
    "    minutes = int((secs % 3600) // 60)\n",
    "    seconds = int(secs % 60)\n",
    "    return \"{:02}:{:02}:{:02}:{:02}.{}\".format(days, hours, minutes, seconds, \"{:.3}\".format(ms)[2:])\n",
    "\n",
    "def timer(f, *args):\n",
    "    print_and_log(\"Start: {}\".format(curr_time()))\n",
    "    start = time.time()\n",
    "    result = f(*args)\n",
    "    end = time.time()\n",
    "    print_and_log(\"End: {}\".format(curr_time()))\n",
    "    print_and_log(\"Finished in {}\".format(sec_to_str(end - start)))\n",
    "    return result\n",
    "\n",
    "def preprocess(samples, sample_rate):\n",
    "    padded = np.zeros(sample_rate)\n",
    "    samples = samples[:sample_rate]\n",
    "    padded[:samples.shape[0]] = samples\n",
    "    return padded\n",
    "\n",
    "def select_labels(df, allowed):\n",
    "    return df[df['Label'].isin(allowed)]\n",
    "    \n",
    "def select_categories(df, allowed):\n",
    "    return df[df['Category'].isin(allowed)]\n",
    "\n",
    "def select_channels(df, allowed):\n",
    "    labels = []\n",
    "    for i in range(1, 9):\n",
    "        if i not in allowed:\n",
    "            labels.append(\"Path{}\".format(i))\n",
    "    return df.drop(labels, axis=1)\n",
    "\n",
    "def select_days(df, allowed):\n",
    "    return df[df['Day'].isin(allowed)]\n",
    "\n",
    "def select_months(df, allowed):\n",
    "    return df[df['Month'].isin(allowed)]\n",
    "\n",
    "def select_sets(df, allowed):\n",
    "    return df[df['Set'].isin(allowed)]\n",
    "\n",
    "def remove_voice(df):\n",
    "    return df.drop([\"Path4\"], axis=1)\n",
    "\n",
    "def str_to_l(x):\n",
    "    return [int(n) for n in x if n <= '9' and n >= '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def _parse_function(label, *filenames):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % VERBOSITY == 0:\n",
    "        print_and_log(\"\\tProcessed {}th image\".format(count))\n",
    "    expected_shape = tf.constant([1, INPUT_HEIGHT, INPUT_WIDTH, IMG_CHANNELS])\n",
    "    image = None\n",
    "    for filename in filenames:\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_image(image_string, channels=IMG_CHANNELS)\n",
    "        image_decoded = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "        image_decoded = tf.reshape(image_decoded, expected_shape)\n",
    "        image_decoded = tf.image.rgb_to_grayscale(image_decoded)\n",
    "        if RESIZE:\n",
    "            image_decoded = tf.image.resize_bicubic(image_decoded, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "        if image is not None:\n",
    "            image = tf.concat([image, image_decoded], 3)\n",
    "        else:\n",
    "            image = image_decoded\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    input_layer = tf.reshape(features, [-1, TARGET_HEIGHT, TARGET_WIDTH, len(CHANNELS)])\n",
    "    pool = input_layer\n",
    "\n",
    "    for num_filters in [32, 64]:\n",
    "        conv = tf.layers.conv2d(\n",
    "            inputs=pool,\n",
    "            filters=num_filters,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "        pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool = tf.layers.flatten(pool)\n",
    "    dense = tf.layers.dense(inputs=pool, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=DROPOUT, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_labels)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if not TPU:\n",
    "        tf.summary.histogram(\"predictions\", predictions[\"probabilities\"])\n",
    "        tf.summary.histogram(\"classes\", predictions[\"classes\"])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE, tf.train.get_global_step(), SPP, DECAY_RATE, staircase=True)\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    predictions[\"loss\"] = loss\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        if TPU:\n",
    "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_input_fn(dataset, batch_size, num_epochs=None):\n",
    "    def _input_fn(num_epochs=None, shuffle=True):\n",
    "        ds = dataset.batch(batch_size).repeat(num_epochs)\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(SHUFFLE_SIZE)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn\n",
    "\n",
    "def create_predict_input_fn(dataset, batch_size):\n",
    "    def _input_fn():\n",
    "        ds = dataset.batch(batch_size)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(paths[\"Log\"], 'w') as log:\n",
    "    log.write(make_header(\"Starting Script\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create variables for the paths\n",
    "train_csv = paths[\"Training\"]\n",
    "\n",
    "# Store the labels to train\n",
    "all_labels = LABELS\n",
    "labels = LABELS\n",
    "num_labels = len(labels)\n",
    "labels = {x[1]:x[0] for x in enumerate(labels)}\n",
    "reverse_lookup = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "           MAKING TRAINING DATA           \n",
      "##########################################\n",
      "##########################################\n",
      "                TRAIN DATA                \n",
      "##########################################\n",
      "           Day    Month         Label  SequenceNumber  Path9  Path10  Path11  \\\n",
      "count  10115.0  10115.0  10115.000000    10115.000000    0.0     0.0     0.0   \n",
      "mean       6.0     12.0      0.499753     5054.501236    NaN     NaN     NaN   \n",
      "std        0.0      0.0      0.500025     2920.094391    NaN     NaN     NaN   \n",
      "min        6.0     12.0      0.000000        0.000000    NaN     NaN     NaN   \n",
      "25%        6.0     12.0      0.000000     2524.000000    NaN     NaN     NaN   \n",
      "50%        6.0     12.0      0.000000     5053.000000    NaN     NaN     NaN   \n",
      "75%        6.0     12.0      1.000000     7582.500000    NaN     NaN     NaN   \n",
      "max        6.0     12.0      1.000000    10114.000000    NaN     NaN     NaN   \n",
      "\n",
      "       Path12  Path13  Path14  Path15  Path16  Path17  Path18  Path19  Path20  \\\n",
      "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "       Path21  Path22  \n",
      "count     0.0     0.0  \n",
      "mean      NaN     NaN  \n",
      "std       NaN     NaN  \n",
      "min       NaN     NaN  \n",
      "25%       NaN     NaN  \n",
      "50%       NaN     NaN  \n",
      "75%       NaN     NaN  \n",
      "max       NaN     NaN  \n",
      "   Category  Day  Month  Label  SequenceNumber         Set  \\\n",
      "0  no_voice    6     12      0            5343  Validation   \n",
      "1  no_voice    6     12      1             224     Testing   \n",
      "2  no_voice    6     12      0            6904     Testing   \n",
      "3  no_voice    6     12      0            9012    Training   \n",
      "4  no_voice    6     12      0            2264     Testing   \n",
      "5  no_voice    6     12      1            5672    Training   \n",
      "6  no_voice    6     12      1            3023  Validation   \n",
      "7  no_voice    6     12      1            1780    Training   \n",
      "8  no_voice    6     12      1            3583  Validation   \n",
      "9  no_voice    6     12      0            3973  Validation   \n",
      "\n",
      "                                               Path1  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "\n",
      "                                               Path2  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "\n",
      "                                               Path3  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "\n",
      "                                               Path4   ...    Path13  Path14  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...       NaN     NaN   \n",
      "\n",
      "   Path15  Path16  Path17  Path18  Path19  Path20  Path21  Path22  \n",
      "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "5     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "6     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "7     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "9     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[10 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make the training data\n",
    "print_and_log_header(\"MAKING TRAINING DATA\")\n",
    "train_data = pd.read_csv(train_csv)\n",
    "\n",
    "# Filter the training data\n",
    "train_data = select_categories(train_data, CATEGORY)\n",
    "train_data = select_channels(train_data, CHANNELS)\n",
    "train_data = select_labels(train_data, LABELS)\n",
    "train_data = select_months(train_data, MONTHS)\n",
    "train_data = select_days(train_data, DAYS)\n",
    "# train_data = remove_voice(train_data)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "tdcopy = pd.DataFrame(train_data)\n",
    "train_data[\"Label\"] = train_data[\"Label\"].map(labels)\n",
    "\n",
    "if VERBOSE:\n",
    "    print_and_log_header(\"TRAIN DATA\")\n",
    "    print_and_log(train_data.describe())\n",
    "    print_and_log(train_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Separate Labels\n",
    "train_labels = train_data.pop(target_label)\n",
    "img_paths = [\"Path{}\".format(channel) for channel in CHANNELS]\n",
    "train_data = train_data[img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "          Parsing Training Data           \n",
      "##########################################\n",
      "Start: 2018-12-08 07:45:25.531684\n",
      "End: 2018-12-08 07:45:25.834503\n",
      "Finished in 00:00:00:00.302\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Vectors of filenames.\n",
    "t_f, v_f, s_f = [], [], []\n",
    "for i in range(1, 1 + len(CHANNELS)):\n",
    "    channel = CHANNELS[i-1]\n",
    "    l = \"Path{}\".format(channel)\n",
    "    t_f.append(tf.constant(train_data[l]))\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i]\n",
    "# Vectors of labels\n",
    "train_labels = tf.constant(train_labels)\n",
    "\n",
    "# Make datasets from filenames and labels\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_labels, *t_f))\n",
    "print_and_log_header(\"Parsing Training Data\")\n",
    "train_data = timer(lambda: train_data.map(_parse_function))\n",
    "print_and_log(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 TRAINING                 \n",
      "##########################################\n",
      "<MapDataset shapes: ((1, 28, 28, 4), ()), types: (tf.float32, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print_and_log_header(\"TRAINING\")\n",
    "print_and_log(train_data)\n",
    "print_and_log(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=paths[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the input functions.\n",
    "training_eval_input_fn = create_predict_input_fn(train_data, DEFAULT_BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Prediction  Probability  Category  Day  Month  Label  SequenceNumber  \\\n",
      "0               0     0.515244  no_voice    6     12      0            5343   \n",
      "1               0     0.520391  no_voice    6     12      1             224   \n",
      "2               0     0.512518  no_voice    6     12      0            6904   \n",
      "3               0     0.513383  no_voice    6     12      0            9012   \n",
      "4               0     0.519682  no_voice    6     12      0            2264   \n",
      "5               0     0.517028  no_voice    6     12      1            5672   \n",
      "6               0     0.519126  no_voice    6     12      1            3023   \n",
      "7               0     0.510309  no_voice    6     12      1            1780   \n",
      "8               0     0.518216  no_voice    6     12      1            3583   \n",
      "9               0     0.515283  no_voice    6     12      0            3973   \n",
      "10              0     0.513227  no_voice    6     12      0            2510   \n",
      "11              0     0.522894  no_voice    6     12      0            2593   \n",
      "12              0     0.517564  no_voice    6     12      0             193   \n",
      "13              0     0.509318  no_voice    6     12      0            3053   \n",
      "14              0     0.511969  no_voice    6     12      0            3191   \n",
      "15              0     0.512153  no_voice    6     12      1            4302   \n",
      "16              0     0.521298  no_voice    6     12      1            1821   \n",
      "17              0     0.515553  no_voice    6     12      0            9860   \n",
      "18              0     0.518007  no_voice    6     12      1            9293   \n",
      "19              0     0.516283  no_voice    6     12      0            3602   \n",
      "20              0     0.521050  no_voice    6     12      1            5040   \n",
      "21              0     0.513723  no_voice    6     12      1            4642   \n",
      "22              0     0.520060  no_voice    6     12      0            6980   \n",
      "23              0     0.518326  no_voice    6     12      0            7733   \n",
      "24              0     0.512665  no_voice    6     12      1            6073   \n",
      "25              0     0.515112  no_voice    6     12      0            4180   \n",
      "26              0     0.518122  no_voice    6     12      0            3193   \n",
      "27              0     0.515748  no_voice    6     12      1            6960   \n",
      "28              0     0.518815  no_voice    6     12      1            6923   \n",
      "29              0     0.521354  no_voice    6     12      1            4030   \n",
      "...           ...          ...       ...  ...    ...    ...             ...   \n",
      "10085           0     0.516882  no_voice    6     12      0            5411   \n",
      "10086           0     0.513091  no_voice    6     12      0            2890   \n",
      "10087           0     0.518990  no_voice    6     12      1            5260   \n",
      "10088           0     0.511154  no_voice    6     12      0            6263   \n",
      "10089           0     0.521737  no_voice    6     12      1            6514   \n",
      "10090           0     0.521577  no_voice    6     12      1            3591   \n",
      "10091           0     0.513350  no_voice    6     12      0            4930   \n",
      "10092           0     0.524398  no_voice    6     12      0            2344   \n",
      "10093           0     0.518700  no_voice    6     12      0            6200   \n",
      "10094           0     0.519462  no_voice    6     12      0            2590   \n",
      "10095           0     0.516341  no_voice    6     12      1            1983   \n",
      "10096           0     0.520414  no_voice    6     12      0            2082   \n",
      "10097           0     0.519664  no_voice    6     12      1            1564   \n",
      "10098           0     0.510891  no_voice    6     12      0            1090   \n",
      "10099           0     0.510435  no_voice    6     12      0            3402   \n",
      "10100           0     0.519727  no_voice    6     12      0            3024   \n",
      "10101           0     0.510393  no_voice    6     12      0            1573   \n",
      "10102           0     0.513397  no_voice    6     12      1             304   \n",
      "10103           0     0.519340  no_voice    6     12      0            4604   \n",
      "10104           0     0.515820  no_voice    6     12      0            6411   \n",
      "10105           0     0.516883  no_voice    6     12      0            2830   \n",
      "10106           0     0.512674  no_voice    6     12      1            2412   \n",
      "10107           0     0.517498  no_voice    6     12      0            1763   \n",
      "10108           0     0.515154  no_voice    6     12      0            8244   \n",
      "10109           0     0.512833  no_voice    6     12      1            9792   \n",
      "10110           0     0.518074  no_voice    6     12      1             654   \n",
      "10111           0     0.521121  no_voice    6     12      1            1320   \n",
      "10112           0     0.515316  no_voice    6     12      0            5041   \n",
      "10113           0     0.515098  no_voice    6     12      0            6961   \n",
      "10114           0     0.518893  no_voice    6     12      1            5083   \n",
      "\n",
      "              Set                                              Path1  \\\n",
      "0      Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "1         Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "2         Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "3        Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "4         Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "5        Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "6      Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "7        Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "8      Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "9      Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "11     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "12     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "13     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "14       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "15       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "16       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "17       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "18     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "19       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "20       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "21       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "22       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "23     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "24     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "25       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "26     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "27       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "28     Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "29       Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "...           ...                                                ...   \n",
      "10085    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10086    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10087    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10088  Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10089     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10090    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10091    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10092     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10093    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10094    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10095  Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10096    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10097     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10098    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10099    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10100     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10101  Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10102     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10103     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10104    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10105    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10106    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10107  Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10108     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10109    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10110     Testing  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10111    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10112    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10113    Training  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "10114  Validation  /Users/kyy/cerebro_train/images_scaled/taylor_...   \n",
      "\n",
      "                                                   Path2   ...   Path13  \\\n",
      "0      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "1      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "2      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "3      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "4      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "5      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "6      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "7      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "8      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "9      /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "11     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "12     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "13     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "14     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "15     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "16     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "17     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "18     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "19     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "20     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "21     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "22     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "23     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "24     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "25     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "26     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "27     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "28     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "29     /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "...                                                  ...   ...      ...   \n",
      "10085  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10086  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10087  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10088  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10089  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10090  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10091  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10092  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10093  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10094  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10095  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10096  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10097  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10098  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10099  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10100  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10101  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10102  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10103  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10104  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10105  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10106  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10107  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10108  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10109  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10110  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10111  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10112  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10113  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "10114  /Users/kyy/cerebro_train/images_scaled/taylor_...   ...      NaN   \n",
      "\n",
      "      Path14  Path15  Path16  Path17  Path18  Path19  Path20  Path21  Path22  \n",
      "0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "5        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "6        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "7        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "9        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "11       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "12       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "13       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "14       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "15       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "16       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "17       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "18       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "19       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "20       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "21       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "22       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "23       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "24       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "25       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "26       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "27       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "28       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "29       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
      "10085    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10086    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10087    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10088    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10089    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10090    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10091    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10092    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10093    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10094    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10095    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10096    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10097    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10098    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10099    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10100    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10101    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10102    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10103    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10104    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10105    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10106    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10107    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10108    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10109    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10110    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10111    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10112    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10113    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10114    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[10115 rows x 26 columns]\n",
      "##########################################\n",
      "                 OUTLIERS                 \n",
      "##########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>SequenceNumber</th>\n",
       "      <th>Path9</th>\n",
       "      <th>Path10</th>\n",
       "      <th>Path11</th>\n",
       "      <th>Path12</th>\n",
       "      <th>Path13</th>\n",
       "      <th>Path14</th>\n",
       "      <th>Path15</th>\n",
       "      <th>Path16</th>\n",
       "      <th>Path17</th>\n",
       "      <th>Path18</th>\n",
       "      <th>Path19</th>\n",
       "      <th>Path20</th>\n",
       "      <th>Path21</th>\n",
       "      <th>Path22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1011.000000</td>\n",
       "      <td>1011.000000</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1011.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.027695</td>\n",
       "      <td>0.508329</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5193.369931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.164180</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3242.277203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500041</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507452</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2113.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508984</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5193.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509917</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8352.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10114.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prediction  Probability     Day   Month  SequenceNumber  Path9  \\\n",
       "count  1011.000000  1011.000000  1011.0  1011.0     1011.000000    0.0   \n",
       "mean      0.027695     0.508329     6.0    12.0     5193.369931    NaN   \n",
       "std       0.164180     0.002287     0.0     0.0     3242.277203    NaN   \n",
       "min       0.000000     0.500041     6.0    12.0        1.000000    NaN   \n",
       "25%       0.000000     0.507452     6.0    12.0     2113.500000    NaN   \n",
       "50%       0.000000     0.508984     6.0    12.0     5193.000000    NaN   \n",
       "75%       0.000000     0.509917     6.0    12.0     8352.000000    NaN   \n",
       "max       1.000000     0.510738     6.0    12.0    10114.000000    NaN   \n",
       "\n",
       "       Path10  Path11  Path12  Path13  Path14  Path15  Path16  Path17  Path18  \\\n",
       "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Path19  Path20  Path21  Path22  \n",
       "count     0.0     0.0     0.0     0.0  \n",
       "mean      NaN     NaN     NaN     NaN  \n",
       "std       NaN     NaN     NaN     NaN  \n",
       "min       NaN     NaN     NaN     NaN  \n",
       "25%       NaN     NaN     NaN     NaN  \n",
       "50%       NaN     NaN     NaN     NaN  \n",
       "75%       NaN     NaN     NaN     NaN  \n",
       "max       NaN     NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 KEEPERS                  \n",
      "##########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>SequenceNumber</th>\n",
       "      <th>Path9</th>\n",
       "      <th>Path10</th>\n",
       "      <th>Path11</th>\n",
       "      <th>Path12</th>\n",
       "      <th>Path13</th>\n",
       "      <th>Path14</th>\n",
       "      <th>Path15</th>\n",
       "      <th>Path16</th>\n",
       "      <th>Path17</th>\n",
       "      <th>Path18</th>\n",
       "      <th>Path19</th>\n",
       "      <th>Path20</th>\n",
       "      <th>Path21</th>\n",
       "      <th>Path22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9104.0</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.0</td>\n",
       "      <td>9104.0</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517253</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5039.079855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2881.878250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510741</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514560</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2560.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517072</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5041.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.519689</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7531.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.529993</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10084.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prediction  Probability     Day   Month  SequenceNumber  Path9  Path10  \\\n",
       "count      9104.0  9104.000000  9104.0  9104.0     9104.000000    0.0     0.0   \n",
       "mean          0.0     0.517253     6.0    12.0     5039.079855    NaN     NaN   \n",
       "std           0.0     0.003521     0.0     0.0     2881.878250    NaN     NaN   \n",
       "min           0.0     0.510741     6.0    12.0        0.000000    NaN     NaN   \n",
       "25%           0.0     0.514560     6.0    12.0     2560.000000    NaN     NaN   \n",
       "50%           0.0     0.517072     6.0    12.0     5041.000000    NaN     NaN   \n",
       "75%           0.0     0.519689     6.0    12.0     7531.250000    NaN     NaN   \n",
       "max           0.0     0.529993     6.0    12.0    10084.000000    NaN     NaN   \n",
       "\n",
       "       Path11  Path12  Path13  Path14  Path15  Path16  Path17  Path18  Path19  \\\n",
       "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Path20  Path21  Path22  \n",
       "count     0.0     0.0     0.0  \n",
       "mean      NaN     NaN     NaN  \n",
       "std       NaN     NaN     NaN  \n",
       "min       NaN     NaN     NaN  \n",
       "25%       NaN     NaN     NaN  \n",
       "50%       NaN     NaN     NaN  \n",
       "75%       NaN     NaN     NaN  \n",
       "max       NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create predicitons and remove 10% lowest confidence rows\n",
    "results = [x for x in classifier.predict(input_fn=training_eval_input_fn)]\n",
    "classes = [x[\"classes\"] for x in results]\n",
    "probs = [x[\"probabilities\"] for x in results]\n",
    "probs = pd.Series(probs)\n",
    "probs = probs.apply(lambda x: max(x))\n",
    "tdf = pd.DataFrame({\"Prediction\":classes, \"Probability\":probs})\n",
    "num_items = tdf.shape[0]\n",
    "for k in tdcopy:\n",
    "    tdf[k] = tdcopy[k]\n",
    "outliers = tdf.nsmallest(int(num_items * OUTLIER_PERCENTAGE), \"Probability\")\n",
    "keepers = tdf.append(outliers, ignore_index=True).drop_duplicates([\"Day\", \"Month\", \"Label\", \"SequenceNumber\"], keep=False).reset_index(drop=True)\n",
    "print(tdf)\n",
    "outliers = outliers.reset_index(drop=True)\n",
    "keepers[\"Label\"] = keepers[\"Label\"].apply(lambda x: reverse_lookup[x])\n",
    "outliers[\"Label\"] = outliers[\"Label\"].apply(lambda x: reverse_lookup[x])\n",
    "if DISPLAY:\n",
    "    print_and_log_header(\"OUTLIERS\")\n",
    "    display.display(outliers.describe())\n",
    "    print_and_log_header(\"KEEPERS\")\n",
    "    display.display(keepers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "keepers.drop([\"Prediction\", \"Probability\"], axis=1, inplace=True)\n",
    "outliers.drop([\"Prediction\", \"Probability\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIERS\n",
      "\tTraining:\t588\n",
      "\tValidation:\t221\n",
      "\tTesting:\t202\n"
     ]
    }
   ],
   "source": [
    "s = outliers[\"Set\"]\n",
    "tra = sum(s.apply(lambda x: 1 if x == \"Training\" else 0))\n",
    "val = sum(s.apply(lambda x: 1 if x == \"Validation\" else 0))\n",
    "tst = sum(s.apply(lambda x: 1 if x == \"Testing\" else 0))\n",
    "print(\"OUTLIERS\\n\\tTraining:\\t{}\\n\\tValidation:\\t{}\\n\\tTesting:\\t{}\".format(tra, val, tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KEEPERS to 'keepers_12_06_1234.csv'\n",
      "Saved OUTLIERS to 'outliers_12_06_1234.csv'\n"
     ]
    }
   ],
   "source": [
    "KEEPERS_NAME = \"keepers_{:02}_{:02}_{}.csv\".format(MONTHS[0], DAYS[0], NUMS)\n",
    "OUTLIERS_NAME = \"outliers_{:02}_{:02}_{}.csv\".format(MONTHS[0], DAYS[0], NUMS)\n",
    "keepers.to_csv(KEEPERS_NAME)\n",
    "print(\"Saved KEEPERS to '{}'\".format(KEEPERS_NAME))\n",
    "outliers.to_csv(OUTLIERS_NAME)\n",
    "print(\"Saved OUTLIERS to '{}'\".format(OUTLIERS_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
