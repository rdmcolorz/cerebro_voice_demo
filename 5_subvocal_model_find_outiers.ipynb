{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kyy/cerebro_train/NONVOCAL_RUNS_YN_12_11/models/NONVOCAL_RUNS_YN_12_11_osc_norm/model_dir_1234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from IPython import display\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# NOTES\n",
    "NOTES = \"28x28\"\n",
    "\n",
    "# VARS\n",
    "target_label = \"Label\"\n",
    "id_label = \"fname\"\n",
    "VERBOSE = True\n",
    "DISPLAY = True\n",
    "TEST = False\n",
    "TPU = False\n",
    "RESIZE = True\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "TARGET_WIDTH = 28 if RESIZE else INPUT_WIDTH\n",
    "TARGET_HEIGHT = 28 if RESIZE else INPUT_HEIGHT\n",
    "DECAY_RATE = 0.9\n",
    "IMG_CHANNELS = 3\n",
    "DROPOUT = 0.4\n",
    "TYPE = \"CNN\"\n",
    "DEFAULT_BS = 128 # default batch size\n",
    "UNK_DROP_RATE = 1.0 # drop 100% of unknown categories\n",
    "OUTLIER_PERCENTAGE = 0.2\n",
    "\n",
    "FNAME = \"osc_test\"\n",
    "CATEGORY = [\"no_voice\"]\n",
    "LABELS = [\"lights-on\", \"turn-off\", \"---\"]\n",
    "CHANNELS = [1, 2, 3, 4]\n",
    "NUMS = ''.join([str(x) for x in CHANNELS])\n",
    "MONTHS = [12]\n",
    "DAYS = [11,12,13,14]\n",
    "\n",
    "if TEST:\n",
    "    LEARNING_STEPS = 100\n",
    "    SPP = 4\n",
    "    LEARNING_RATE = .05\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSITY = 1000\n",
    "    TEST_SIZE = 1000\n",
    "    SHUFFLE_SIZE = 64\n",
    "else:\n",
    "    LEARNING_STEPS = 5000\n",
    "    SPP = 200\n",
    "    LEARNING_RATE = .025\n",
    "    BATCH_SIZE = 64\n",
    "    VERBOSITY = 1000\n",
    "    SHUFFLE_SIZE = 256\n",
    "\n",
    "def curr_time():\n",
    "    return datetime.now() - timedelta(hours=7) # offset from UTC to PST\n",
    "\n",
    "ROOT = os.getcwd() + \"/\"\n",
    "if CATEGORY[0] == \"no_voice\":\n",
    "    RUN_ROOT = ROOT+\"NONVOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "else:\n",
    "    RUN_ROOT = ROOT+\"VOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "RUN_ROOT_LOG = RUN_ROOT+\"logs/\"\n",
    "\n",
    "# PATHS\n",
    "paths = {\n",
    "    \"Training\":ROOT+\"train_csv/\" + FNAME + \".csv\",\n",
    "    \"Model\":RUN_ROOT+\"models/NONVOCAL_RUNS_YN_12_11_osc_norm/model_dir_1234\",# model_dir_{}/.format(NUMS),\n",
    "    \"Logs\":RUN_ROOT_LOG+\"{}_{}/\".format(NUMS, datetime.strftime(curr_time(), \"%b%d%Y_%H%M%S\"))\n",
    "}\n",
    "paths[\"Log\"] = paths[\"Logs\"] + \"log.txt\"\n",
    "if not os.path.isdir(RUN_ROOT):\n",
    "    os.mkdir(RUN_ROOT)\n",
    "if not os.path.isdir(RUN_ROOT_LOG):\n",
    "    os.mkdir(RUN_ROOT_LOG)\n",
    "if not os.path.isdir(paths[\"Logs\"]):\n",
    "    os.mkdir(paths[\"Logs\"])\n",
    "    \n",
    "print(paths['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_header(s):\n",
    "    return (\"#\" * 42) + (\"\\n{:^42}\\n\".format(s)) + (\"#\" * 42)\n",
    "    \n",
    "def print_and_log(s):\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(str(s))\n",
    "        log.write(\"\\n\")\n",
    "    print(s)\n",
    "        \n",
    "def print_and_log_header(s):\n",
    "    h = make_header(str(s))\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(h)\n",
    "        log.write(\"\\n\")\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sec_to_str(secs):\n",
    "    ms = secs - int(secs)\n",
    "    days = int(secs // (24 * 3600))\n",
    "    hours = int((secs % ((24 * 3600))) // 3600)\n",
    "    minutes = int((secs % 3600) // 60)\n",
    "    seconds = int(secs % 60)\n",
    "    return \"{:02}:{:02}:{:02}:{:02}.{}\".format(days, hours, minutes, seconds, \"{:.3}\".format(ms)[2:])\n",
    "\n",
    "def timer(f, *args):\n",
    "    print_and_log(\"Start: {}\".format(curr_time()))\n",
    "    start = time.time()\n",
    "    result = f(*args)\n",
    "    end = time.time()\n",
    "    print_and_log(\"End: {}\".format(curr_time()))\n",
    "    print_and_log(\"Finished in {}\".format(sec_to_str(end - start)))\n",
    "    return result\n",
    "\n",
    "def preprocess(samples, sample_rate):\n",
    "    padded = np.zeros(sample_rate)\n",
    "    samples = samples[:sample_rate]\n",
    "    padded[:samples.shape[0]] = samples\n",
    "    return padded\n",
    "\n",
    "def select_labels(df, allowed):\n",
    "    return df[df['Label'].isin(allowed)]\n",
    "    \n",
    "def select_categories(df, allowed):\n",
    "    return df[df['Category'].isin(allowed)]\n",
    "\n",
    "def select_channels(df, allowed):\n",
    "    labels = []\n",
    "    for i in range(1, 5):\n",
    "        if i not in allowed:\n",
    "            labels.append(\"Path{}\".format(i))\n",
    "    return df.drop(labels, axis=1)\n",
    "\n",
    "def select_days(df, allowed):\n",
    "    return df[df['Day'].isin(allowed)]\n",
    "\n",
    "def select_months(df, allowed):\n",
    "    return df[df['Month'].isin(allowed)]\n",
    "\n",
    "def select_sets(df, allowed):\n",
    "    return df[df['Set'].isin(allowed)]\n",
    "\n",
    "def remove_voice(df):\n",
    "    return df.drop([\"Path4\"], axis=1)\n",
    "\n",
    "def str_to_l(x):\n",
    "    return [int(n) for n in x if n <= '9' and n >= '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def _parse_function(label, *filenames):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % VERBOSITY == 0:\n",
    "        print_and_log(\"\\tProcessed {}th image\".format(count))\n",
    "    expected_shape = tf.constant([1, INPUT_HEIGHT, INPUT_WIDTH, IMG_CHANNELS])\n",
    "    image = None\n",
    "    for filename in filenames:\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_image(image_string, channels=IMG_CHANNELS)\n",
    "        image_decoded = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "        image_decoded = tf.reshape(image_decoded, expected_shape)\n",
    "        image_decoded = tf.image.rgb_to_grayscale(image_decoded)\n",
    "        if RESIZE:\n",
    "            image_decoded = tf.image.resize_bicubic(image_decoded, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "        if image is not None:\n",
    "            image = tf.concat([image, image_decoded], 3)\n",
    "        else:\n",
    "            image = image_decoded\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    input_layer = tf.reshape(features, [-1, TARGET_HEIGHT, TARGET_WIDTH, len(CHANNELS)])\n",
    "    pool = input_layer\n",
    "\n",
    "    for num_filters in [32, 64]:\n",
    "        conv = tf.layers.conv2d(\n",
    "            inputs=pool,\n",
    "            filters=num_filters,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "        pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool = tf.layers.flatten(pool)\n",
    "    dense = tf.layers.dense(inputs=pool, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=DROPOUT, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_labels)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if not TPU:\n",
    "        tf.summary.histogram(\"predictions\", predictions[\"probabilities\"])\n",
    "        tf.summary.histogram(\"classes\", predictions[\"classes\"])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE, tf.train.get_global_step(), SPP, DECAY_RATE, staircase=True)\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    predictions[\"loss\"] = loss\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        if TPU:\n",
    "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_input_fn(dataset, batch_size, num_epochs=None):\n",
    "    def _input_fn(num_epochs=None, shuffle=True):\n",
    "        ds = dataset.batch(batch_size).repeat(num_epochs)\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(SHUFFLE_SIZE)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn\n",
    "\n",
    "def create_predict_input_fn(dataset, batch_size):\n",
    "    def _input_fn():\n",
    "        ds = dataset.batch(batch_size)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(paths[\"Log\"], 'w') as log:\n",
    "    log.write(make_header(\"Starting Script\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create variables for the paths\n",
    "train_csv = paths[\"Training\"]\n",
    "\n",
    "# Store the labels to train\n",
    "all_labels = LABELS\n",
    "labels = LABELS\n",
    "num_labels = len(labels)\n",
    "labels = {x[1]:x[0] for x in enumerate(labels)}\n",
    "reverse_lookup = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "           MAKING TRAINING DATA           \n",
      "##########################################\n",
      "##########################################\n",
      "                TRAIN DATA                \n",
      "##########################################\n",
      "                Day    Month         Label  SequenceNumber\n",
      "count  21072.000000  21072.0  21072.000000    21072.000000\n",
      "mean      12.500000     12.0      0.500285     1317.541002\n",
      "std        1.119842      0.0      0.500012      760.459106\n",
      "min       11.000000     12.0      0.000000        1.000000\n",
      "25%       11.000000     12.0      0.000000      659.000000\n",
      "50%       12.000000     12.0      1.000000     1317.500000\n",
      "75%       14.000000     12.0      1.000000     1976.000000\n",
      "max       14.000000     12.0      1.000000     2652.000000\n",
      "   Category  Day  Month  Label  SequenceNumber         Set  \\\n",
      "0  no_voice   14     12      0             363    Training   \n",
      "1  no_voice   13     12      0             201    Training   \n",
      "2  no_voice   14     12      0            1168  Validation   \n",
      "3  no_voice   12     12      1             774    Training   \n",
      "4  no_voice   13     12      1             893    Training   \n",
      "5  no_voice   11     12      0             108  Validation   \n",
      "6  no_voice   12     12      0             700    Training   \n",
      "7  no_voice   11     12      0              75    Training   \n",
      "8  no_voice   11     12      0             110    Training   \n",
      "9  no_voice   14     12      0             230    Training   \n",
      "\n",
      "                                               Path1  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                               Path2  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                               Path3  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                               Path4  \n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n"
     ]
    }
   ],
   "source": [
    "# Make the training data\n",
    "print_and_log_header(\"MAKING TRAINING DATA\")\n",
    "train_data = pd.read_csv(train_csv)\n",
    "\n",
    "# Filter the training data\n",
    "train_data = select_categories(train_data, CATEGORY)\n",
    "train_data = select_channels(train_data, CHANNELS)\n",
    "train_data = select_labels(train_data, LABELS)\n",
    "train_data = select_months(train_data, MONTHS)\n",
    "train_data = select_days(train_data, DAYS)\n",
    "# train_data = remove_voice(train_data)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "tdcopy = pd.DataFrame(train_data)\n",
    "train_data[\"Label\"] = train_data[\"Label\"].map(labels)\n",
    "\n",
    "if VERBOSE:\n",
    "    print_and_log_header(\"TRAIN DATA\")\n",
    "    print_and_log(train_data.describe())\n",
    "    print_and_log(train_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Separate Labels\n",
    "train_labels = train_data.pop(target_label)\n",
    "img_paths = [\"Path{}\".format(channel) for channel in CHANNELS]\n",
    "train_data = train_data[img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "          Parsing Training Data           \n",
      "##########################################\n",
      "Start: 2018-12-16 09:20:28.766471\n",
      "End: 2018-12-16 09:20:29.018939\n",
      "Finished in 00:00:00:00.247\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Vectors of filenames.\n",
    "t_f, v_f, s_f = [], [], []\n",
    "for i in range(1, 1 + len(CHANNELS)):\n",
    "    channel = CHANNELS[i-1]\n",
    "    l = \"Path{}\".format(channel)\n",
    "    t_f.append(tf.constant(train_data[l]))\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i]\n",
    "# Vectors of labels\n",
    "train_labels = tf.constant(train_labels)\n",
    "\n",
    "# Make datasets from filenames and labels\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_labels, *t_f))\n",
    "print_and_log_header(\"Parsing Training Data\")\n",
    "train_data = timer(lambda: train_data.map(_parse_function))\n",
    "print_and_log(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 TRAINING                 \n",
      "##########################################\n",
      "<MapDataset shapes: ((1, 28, 28, 4), ()), types: (tf.float32, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print_and_log_header(\"TRAINING\")\n",
    "print_and_log(train_data)\n",
    "print_and_log(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=paths[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the input functions.\n",
    "training_eval_input_fn = create_predict_input_fn(train_data, DEFAULT_BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Prediction  Probability  Category  Day  Month  Label  SequenceNumber  \\\n",
      "0               2     0.345997  no_voice   14     12      0             363   \n",
      "1               1     0.339042  no_voice   13     12      0             201   \n",
      "2               1     0.340248  no_voice   14     12      0            1168   \n",
      "3               2     0.340250  no_voice   12     12      1             774   \n",
      "4               2     0.344529  no_voice   13     12      1             893   \n",
      "5               2     0.346104  no_voice   11     12      0             108   \n",
      "6               2     0.345354  no_voice   12     12      0             700   \n",
      "7               2     0.343135  no_voice   11     12      0              75   \n",
      "8               2     0.346995  no_voice   11     12      0             110   \n",
      "9               2     0.343151  no_voice   14     12      0             230   \n",
      "10              2     0.343613  no_voice   13     12      1             513   \n",
      "11              2     0.346127  no_voice   13     12      1             942   \n",
      "12              2     0.340749  no_voice   12     12      1            1668   \n",
      "13              2     0.341009  no_voice   14     12      0            1306   \n",
      "14              2     0.340271  no_voice   12     12      0             584   \n",
      "15              2     0.343597  no_voice   13     12      1             266   \n",
      "16              2     0.343699  no_voice   12     12      0            1663   \n",
      "17              2     0.346984  no_voice   12     12      1             665   \n",
      "18              2     0.343262  no_voice   12     12      0            2231   \n",
      "19              2     0.347381  no_voice   13     12      1             273   \n",
      "20              1     0.339292  no_voice   11     12      0             508   \n",
      "21              2     0.341838  no_voice   11     12      1            1387   \n",
      "22              2     0.346788  no_voice   12     12      0             701   \n",
      "23              2     0.342556  no_voice   14     12      0            1188   \n",
      "24              2     0.341024  no_voice   14     12      0            1431   \n",
      "25              2     0.341961  no_voice   13     12      1            2116   \n",
      "26              2     0.342989  no_voice   14     12      1            1093   \n",
      "27              2     0.344397  no_voice   12     12      0             577   \n",
      "28              2     0.345294  no_voice   12     12      1            1685   \n",
      "29              2     0.344874  no_voice   14     12      0             186   \n",
      "...           ...          ...       ...  ...    ...    ...             ...   \n",
      "21042           2     0.339842  no_voice   13     12      0            1547   \n",
      "21043           2     0.345371  no_voice   12     12      0              48   \n",
      "21044           2     0.344915  no_voice   12     12      1             641   \n",
      "21045           2     0.344792  no_voice   11     12      1            1697   \n",
      "21046           2     0.343739  no_voice   11     12      0            1639   \n",
      "21047           2     0.342504  no_voice   11     12      0             321   \n",
      "21048           2     0.342377  no_voice   13     12      1            1087   \n",
      "21049           2     0.342050  no_voice   13     12      1            1446   \n",
      "21050           2     0.340340  no_voice   14     12      1            2295   \n",
      "21051           2     0.339947  no_voice   11     12      0            1127   \n",
      "21052           2     0.343565  no_voice   13     12      1            1501   \n",
      "21053           2     0.342531  no_voice   14     12      1             294   \n",
      "21054           2     0.348391  no_voice   12     12      1             501   \n",
      "21055           2     0.338783  no_voice   13     12      0              20   \n",
      "21056           2     0.342792  no_voice   14     12      1            1822   \n",
      "21057           2     0.338716  no_voice   12     12      1             793   \n",
      "21058           2     0.345377  no_voice   14     12      0             990   \n",
      "21059           2     0.341274  no_voice   12     12      0            1971   \n",
      "21060           2     0.344913  no_voice   12     12      0              62   \n",
      "21061           2     0.344570  no_voice   12     12      1             820   \n",
      "21062           2     0.340501  no_voice   12     12      0            1123   \n",
      "21063           2     0.343006  no_voice   13     12      1            2412   \n",
      "21064           2     0.340318  no_voice   13     12      1            2265   \n",
      "21065           2     0.341819  no_voice   13     12      0             642   \n",
      "21066           2     0.338469  no_voice   11     12      1            1924   \n",
      "21067           2     0.344849  no_voice   11     12      1             608   \n",
      "21068           2     0.339319  no_voice   12     12      1            2390   \n",
      "21069           2     0.346214  no_voice   11     12      1             715   \n",
      "21070           1     0.338710  no_voice   13     12      1            1324   \n",
      "21071           2     0.342264  no_voice   12     12      0            1719   \n",
      "\n",
      "              Set                                              Path1  \\\n",
      "0        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2      Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5      Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9        Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "10       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "11       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "12     Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "13       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "14       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "15       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "16       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "17       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "18       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "19       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "20     Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "22       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "23     Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "24       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "25       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "26       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "27       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "28       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "29       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "...           ...                                                ...   \n",
      "21042    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21043  Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21044    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21045    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21046     Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21047    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21048    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21049    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21050    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21051    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21052    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21053    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21054    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21055    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21056    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21057    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21058    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21059    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21060    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21061    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21062    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21063    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21064    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21065    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21066    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21067  Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21068    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21069    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21070    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21071     Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                                   Path2  \\\n",
      "0      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "10     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "11     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "12     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "13     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "14     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "15     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "16     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "17     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "18     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "19     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "20     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "22     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "23     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "24     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "25     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "26     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "27     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "28     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "29     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "...                                                  ...   \n",
      "21042  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21043  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21044  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21045  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21046  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21047  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21048  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21049  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21050  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21051  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21052  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21053  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21054  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21055  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21056  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21057  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21058  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21059  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21060  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21061  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21062  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21063  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21064  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21065  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21066  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21067  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21068  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21069  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21070  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21071  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                                   Path3  \\\n",
      "0      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9      /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "10     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "11     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "12     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "13     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "14     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "15     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "16     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "17     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "18     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "19     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "20     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "22     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "23     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "24     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "25     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "26     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "27     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "28     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "29     /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "...                                                  ...   \n",
      "21042  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21043  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21044  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21045  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21046  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21047  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21048  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21049  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21050  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21051  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21052  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21053  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21054  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21055  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21056  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21057  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21058  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21059  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21060  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21061  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21062  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21063  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21064  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21065  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21066  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21067  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21068  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21069  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21070  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21071  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                                   Path4  \n",
      "0      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "1      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "2      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "3      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "4      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "5      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "6      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "7      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "8      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "9      /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "10     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "11     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "12     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "13     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "14     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "15     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "16     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "17     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "18     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "19     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "20     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "22     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "23     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "24     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "25     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "26     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "27     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "28     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "29     /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "...                                                  ...  \n",
      "21042  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21043  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21044  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21045  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21046  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21047  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21048  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21049  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21050  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21051  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21052  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21053  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21054  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21055  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21056  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21057  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21058  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21059  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21060  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21061  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21062  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21063  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21064  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21065  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21066  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21067  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21068  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21069  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21070  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "21071  /Users/kyy/cerebro_train/images_scaled/osc_tes...  \n",
      "\n",
      "[21072 rows x 12 columns]\n",
      "##########################################\n",
      "                 OUTLIERS                 \n",
      "##########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>SequenceNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4214.000000</td>\n",
       "      <td>4214.000000</td>\n",
       "      <td>4214.000000</td>\n",
       "      <td>4214.0</td>\n",
       "      <td>4214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.762221</td>\n",
       "      <td>0.339269</td>\n",
       "      <td>12.596108</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1329.951115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.425774</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>1.184256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>746.372269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.335192</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.338769</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>695.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.339433</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.339917</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1957.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.340333</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2652.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prediction  Probability          Day   Month  SequenceNumber\n",
       "count  4214.000000  4214.000000  4214.000000  4214.0     4214.000000\n",
       "mean      1.762221     0.339269    12.596108    12.0     1329.951115\n",
       "std       0.425774     0.000801     1.184256     0.0      746.372269\n",
       "min       1.000000     0.335192    11.000000    12.0        1.000000\n",
       "25%       2.000000     0.338769    11.000000    12.0      695.000000\n",
       "50%       2.000000     0.339433    13.000000    12.0     1330.000000\n",
       "75%       2.000000     0.339917    14.000000    12.0     1957.000000\n",
       "max       2.000000     0.340333    14.000000    12.0     2652.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 KEEPERS                  \n",
      "##########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>SequenceNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16858.000000</td>\n",
       "      <td>16858.000000</td>\n",
       "      <td>16858.000000</td>\n",
       "      <td>16858.0</td>\n",
       "      <td>16858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.980543</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>12.475976</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1314.438842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.138128</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>1.101881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>763.930155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.340334</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.341738</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.343034</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1311.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.344531</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1980.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.352678</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2651.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Prediction   Probability           Day    Month  SequenceNumber\n",
       "count  16858.000000  16858.000000  16858.000000  16858.0    16858.000000\n",
       "mean       1.980543      0.343279     12.475976     12.0     1314.438842\n",
       "std        0.138128      0.001942      1.101881      0.0      763.930155\n",
       "min        1.000000      0.340334     11.000000     12.0        1.000000\n",
       "25%        2.000000      0.341738     12.000000     12.0      653.000000\n",
       "50%        2.000000      0.343034     12.000000     12.0     1311.000000\n",
       "75%        2.000000      0.344531     13.000000     12.0     1980.750000\n",
       "max        2.000000      0.352678     14.000000     12.0     2651.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create predicitons and remove 10% lowest confidence rows\n",
    "results = [x for x in classifier.predict(input_fn=training_eval_input_fn)]\n",
    "classes = [x[\"classes\"] for x in results]\n",
    "probs = [x[\"probabilities\"] for x in results]\n",
    "probs = pd.Series(probs)\n",
    "probs = probs.apply(lambda x: max(x))\n",
    "tdf = pd.DataFrame({\"Prediction\":classes, \"Probability\":probs})\n",
    "num_items = tdf.shape[0]\n",
    "for k in tdcopy:\n",
    "    tdf[k] = tdcopy[k]\n",
    "outliers = tdf.nsmallest(int(num_items * OUTLIER_PERCENTAGE), \"Probability\")\n",
    "keepers = tdf.append(outliers, ignore_index=True).drop_duplicates([\"Day\", \"Month\", \"Label\", \"SequenceNumber\"], keep=False).reset_index(drop=True)\n",
    "print(tdf)\n",
    "outliers = outliers.reset_index(drop=True)\n",
    "keepers[\"Label\"] = keepers[\"Label\"].apply(lambda x: reverse_lookup[x])\n",
    "outliers[\"Label\"] = outliers[\"Label\"].apply(lambda x: reverse_lookup[x])\n",
    "if DISPLAY:\n",
    "    print_and_log_header(\"OUTLIERS\")\n",
    "    display.display(outliers.describe())\n",
    "    print_and_log_header(\"KEEPERS\")\n",
    "    display.display(keepers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "keepers.drop([\"Prediction\", \"Probability\"], axis=1, inplace=True)\n",
    "outliers.drop([\"Prediction\", \"Probability\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIERS\n",
      "\tTraining:\t3358\n",
      "\tValidation:\t413\n",
      "\tTesting:\t443\n"
     ]
    }
   ],
   "source": [
    "s = outliers[\"Set\"]\n",
    "tra = sum(s.apply(lambda x: 1 if x == \"Training\" else 0))\n",
    "val = sum(s.apply(lambda x: 1 if x == \"Validation\" else 0))\n",
    "tst = sum(s.apply(lambda x: 1 if x == \"Testing\" else 0))\n",
    "print(\"OUTLIERS\\n\\tTraining:\\t{}\\n\\tValidation:\\t{}\\n\\tTesting:\\t{}\".format(tra, val, tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KEEPERS to 'keepers_12_11_1234.csv'\n",
      "Saved OUTLIERS to 'outliers_12_11_1234.csv'\n"
     ]
    }
   ],
   "source": [
    "KEEPERS_NAME = \"keepers_{:02}_{:02}_{}.csv\".format(MONTHS[0], DAYS[0], NUMS)\n",
    "OUTLIERS_NAME = \"outliers_{:02}_{:02}_{}.csv\".format(MONTHS[0], DAYS[0], NUMS)\n",
    "keepers.to_csv(KEEPERS_NAME)\n",
    "print(\"Saved KEEPERS to '{}'\".format(KEEPERS_NAME))\n",
    "outliers.to_csv(OUTLIERS_NAME)\n",
    "print(\"Saved OUTLIERS to '{}'\".format(OUTLIERS_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
