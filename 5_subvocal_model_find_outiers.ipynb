{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kyy/cerebro_train/NONVOCAL_RUNS_YN_12_11/models/NONVOCAL_RUNS_YN_12_11/model_dir_1234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from IPython import display\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# NOTES\n",
    "NOTES = \"28x28\"\n",
    "\n",
    "# VARS\n",
    "target_label = \"Label\"\n",
    "id_label = \"fname\"\n",
    "VERBOSE = True\n",
    "DISPLAY = True\n",
    "TEST = False\n",
    "TPU = False\n",
    "RESIZE = True\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "TARGET_WIDTH = 28 if RESIZE else INPUT_WIDTH\n",
    "TARGET_HEIGHT = 28 if RESIZE else INPUT_HEIGHT\n",
    "DECAY_RATE = 0.9\n",
    "IMG_CHANNELS = 3\n",
    "DROPOUT = 0.4\n",
    "TYPE = \"CNN\"\n",
    "DEFAULT_BS = 128 # default batch size\n",
    "UNK_DROP_RATE = 1.0 # drop 100% of unknown categories\n",
    "OUTLIER_PERCENTAGE = 0.1\n",
    "\n",
    "FNAME = \"osc_test2\"\n",
    "CATEGORY = [\"no_voice\"]\n",
    "LABELS = [\"lights-on\", \"turn-off\"]\n",
    "CHANNELS = [1, 2, 3, 4]\n",
    "NUMS = ''.join([str(x) for x in CHANNELS])\n",
    "MONTHS = [12]\n",
    "DAYS = [11,12]\n",
    "\n",
    "if TEST:\n",
    "    LEARNING_STEPS = 100\n",
    "    SPP = 4\n",
    "    LEARNING_RATE = .05\n",
    "    BATCH_SIZE = 32\n",
    "    VERBOSITY = 1000\n",
    "    TEST_SIZE = 1000\n",
    "    SHUFFLE_SIZE = 64\n",
    "else:\n",
    "    LEARNING_STEPS = 5000\n",
    "    SPP = 200\n",
    "    LEARNING_RATE = .025\n",
    "    BATCH_SIZE = 64\n",
    "    VERBOSITY = 1000\n",
    "    SHUFFLE_SIZE = 256\n",
    "\n",
    "def curr_time():\n",
    "    return datetime.now() - timedelta(hours=7) # offset from UTC to PST\n",
    "\n",
    "ROOT = os.getcwd() + \"/\"\n",
    "if CATEGORY[0] == \"no_voice\":\n",
    "    RUN_ROOT = ROOT+\"NONVOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "else:\n",
    "    RUN_ROOT = ROOT+\"VOCAL_RUNS_YN_{:02}_{:02}/\".format(MONTHS[0], DAYS[0])\n",
    "RUN_ROOT_LOG = RUN_ROOT+\"logs/\"\n",
    "\n",
    "# PATHS\n",
    "paths = {\n",
    "    \"Training\":ROOT+\"train_csv/\" + FNAME + \".csv\",\n",
    "    \"Model\":RUN_ROOT+\"models/NONVOCAL_RUNS_YN_12_11/model_dir_1234\",# model_dir_{}/.format(NUMS),\n",
    "    \"Logs\":RUN_ROOT_LOG+\"{}_{}/\".format(NUMS, datetime.strftime(curr_time(), \"%b%d%Y_%H%M%S\"))\n",
    "}\n",
    "paths[\"Log\"] = paths[\"Logs\"] + \"log.txt\"\n",
    "if not os.path.isdir(RUN_ROOT):\n",
    "    os.mkdir(RUN_ROOT)\n",
    "if not os.path.isdir(RUN_ROOT_LOG):\n",
    "    os.mkdir(RUN_ROOT_LOG)\n",
    "if not os.path.isdir(paths[\"Logs\"]):\n",
    "    os.mkdir(paths[\"Logs\"])\n",
    "    \n",
    "print(paths['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_header(s):\n",
    "    return (\"#\" * 42) + (\"\\n{:^42}\\n\".format(s)) + (\"#\" * 42)\n",
    "    \n",
    "def print_and_log(s):\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(str(s))\n",
    "        log.write(\"\\n\")\n",
    "    print(s)\n",
    "        \n",
    "def print_and_log_header(s):\n",
    "    h = make_header(str(s))\n",
    "    with open(paths[\"Log\"], 'a') as log:\n",
    "        log.write(h)\n",
    "        log.write(\"\\n\")\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sec_to_str(secs):\n",
    "    ms = secs - int(secs)\n",
    "    days = int(secs // (24 * 3600))\n",
    "    hours = int((secs % ((24 * 3600))) // 3600)\n",
    "    minutes = int((secs % 3600) // 60)\n",
    "    seconds = int(secs % 60)\n",
    "    return \"{:02}:{:02}:{:02}:{:02}.{}\".format(days, hours, minutes, seconds, \"{:.3}\".format(ms)[2:])\n",
    "\n",
    "def timer(f, *args):\n",
    "    print_and_log(\"Start: {}\".format(curr_time()))\n",
    "    start = time.time()\n",
    "    result = f(*args)\n",
    "    end = time.time()\n",
    "    print_and_log(\"End: {}\".format(curr_time()))\n",
    "    print_and_log(\"Finished in {}\".format(sec_to_str(end - start)))\n",
    "    return result\n",
    "\n",
    "def preprocess(samples, sample_rate):\n",
    "    padded = np.zeros(sample_rate)\n",
    "    samples = samples[:sample_rate]\n",
    "    padded[:samples.shape[0]] = samples\n",
    "    return padded\n",
    "\n",
    "def select_labels(df, allowed):\n",
    "    return df[df['Label'].isin(allowed)]\n",
    "    \n",
    "def select_categories(df, allowed):\n",
    "    return df[df['Category'].isin(allowed)]\n",
    "\n",
    "def select_channels(df, allowed):\n",
    "    labels = []\n",
    "    for i in range(1, 9):\n",
    "        if i not in allowed:\n",
    "            labels.append(\"Path{}\".format(i))\n",
    "    return df.drop(labels, axis=1)\n",
    "\n",
    "def select_days(df, allowed):\n",
    "    return df[df['Day'].isin(allowed)]\n",
    "\n",
    "def select_months(df, allowed):\n",
    "    return df[df['Month'].isin(allowed)]\n",
    "\n",
    "def select_sets(df, allowed):\n",
    "    return df[df['Set'].isin(allowed)]\n",
    "\n",
    "def remove_voice(df):\n",
    "    return df.drop([\"Path4\"], axis=1)\n",
    "\n",
    "def str_to_l(x):\n",
    "    return [int(n) for n in x if n <= '9' and n >= '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def _parse_function(label, *filenames):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % VERBOSITY == 0:\n",
    "        print_and_log(\"\\tProcessed {}th image\".format(count))\n",
    "    expected_shape = tf.constant([1, INPUT_HEIGHT, INPUT_WIDTH, IMG_CHANNELS])\n",
    "    image = None\n",
    "    for filename in filenames:\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_image(image_string, channels=IMG_CHANNELS)\n",
    "        image_decoded = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "        image_decoded = tf.reshape(image_decoded, expected_shape)\n",
    "        image_decoded = tf.image.rgb_to_grayscale(image_decoded)\n",
    "        if RESIZE:\n",
    "            image_decoded = tf.image.resize_bicubic(image_decoded, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "        if image is not None:\n",
    "            image = tf.concat([image, image_decoded], 3)\n",
    "        else:\n",
    "            image = image_decoded\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    input_layer = tf.reshape(features, [-1, TARGET_HEIGHT, TARGET_WIDTH, len(CHANNELS)])\n",
    "    pool = input_layer\n",
    "\n",
    "    for num_filters in [32, 64]:\n",
    "        conv = tf.layers.conv2d(\n",
    "            inputs=pool,\n",
    "            filters=num_filters,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "        pool = tf.layers.max_pooling2d(inputs=conv, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool = tf.layers.flatten(pool)\n",
    "    dense = tf.layers.dense(inputs=pool, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=DROPOUT, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_labels)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if not TPU:\n",
    "        tf.summary.histogram(\"predictions\", predictions[\"probabilities\"])\n",
    "        tf.summary.histogram(\"classes\", predictions[\"classes\"])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE, tf.train.get_global_step(), SPP, DECAY_RATE, staircase=True)\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    predictions[\"loss\"] = loss\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        if TPU:\n",
    "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_input_fn(dataset, batch_size, num_epochs=None):\n",
    "    def _input_fn(num_epochs=None, shuffle=True):\n",
    "        ds = dataset.batch(batch_size).repeat(num_epochs)\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(SHUFFLE_SIZE)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn\n",
    "\n",
    "def create_predict_input_fn(dataset, batch_size):\n",
    "    def _input_fn():\n",
    "        ds = dataset.batch(batch_size)\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(paths[\"Log\"], 'w') as log:\n",
    "    log.write(make_header(\"Starting Script\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create variables for the paths\n",
    "train_csv = paths[\"Training\"]\n",
    "\n",
    "# Store the labels to train\n",
    "all_labels = LABELS\n",
    "labels = LABELS\n",
    "num_labels = len(labels)\n",
    "labels = {x[1]:x[0] for x in enumerate(labels)}\n",
    "reverse_lookup = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "           MAKING TRAINING DATA           \n",
      "##########################################\n",
      "##########################################\n",
      "                TRAIN DATA                \n",
      "##########################################\n",
      "               Day   Month        Label  SequenceNumber  Path9  Path10  \\\n",
      "count  8795.000000  8795.0  8795.000000     8795.000000    0.0     0.0   \n",
      "mean     11.499716    12.0     0.500284     2195.752132    NaN     NaN   \n",
      "std       0.500028     0.0     0.500028     1269.522275    NaN     NaN   \n",
      "min      11.000000    12.0     0.000000        0.000000    NaN     NaN   \n",
      "25%      11.000000    12.0     0.000000     1094.000000    NaN     NaN   \n",
      "50%      11.000000    12.0     1.000000     2194.000000    NaN     NaN   \n",
      "75%      12.000000    12.0     1.000000     3293.500000    NaN     NaN   \n",
      "max      12.000000    12.0     1.000000     4394.000000    NaN     NaN   \n",
      "\n",
      "       Path11  Path12  Path13  Path14  Path15  Path16  Path17  Path18  Path19  \\\n",
      "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "       Path20  Path21  Path22  \n",
      "count     0.0     0.0     0.0  \n",
      "mean      NaN     NaN     NaN  \n",
      "std       NaN     NaN     NaN  \n",
      "min       NaN     NaN     NaN  \n",
      "25%       NaN     NaN     NaN  \n",
      "50%       NaN     NaN     NaN  \n",
      "75%       NaN     NaN     NaN  \n",
      "max       NaN     NaN     NaN  \n",
      "   Category  Day  Month  Label  SequenceNumber         Set  \\\n",
      "0  no_voice   11     12      0             430    Training   \n",
      "1  no_voice   11     12      1            2361    Training   \n",
      "2  no_voice   11     12      0            1241    Training   \n",
      "3  no_voice   12     12      0            3321    Training   \n",
      "4  no_voice   11     12      1            2964     Testing   \n",
      "5  no_voice   12     12      1             974  Validation   \n",
      "6  no_voice   11     12      0            3040    Training   \n",
      "7  no_voice   12     12      1            2023    Training   \n",
      "8  no_voice   12     12      0            1143    Training   \n",
      "9  no_voice   12     12      0            1092    Training   \n",
      "\n",
      "                                               Path1  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                               Path2  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                               Path3  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                               Path4   ...    Path13  Path14  \\\n",
      "0  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "1  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "2  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "3  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "4  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "5  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "6  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "7  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "8  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "9  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...       NaN     NaN   \n",
      "\n",
      "   Path15  Path16  Path17  Path18  Path19  Path20  Path21  Path22  \n",
      "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "5     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "6     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "7     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "9     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[10 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make the training data\n",
    "print_and_log_header(\"MAKING TRAINING DATA\")\n",
    "train_data = pd.read_csv(train_csv)\n",
    "\n",
    "# Filter the training data\n",
    "train_data = select_categories(train_data, CATEGORY)\n",
    "train_data = select_channels(train_data, CHANNELS)\n",
    "train_data = select_labels(train_data, LABELS)\n",
    "train_data = select_months(train_data, MONTHS)\n",
    "train_data = select_days(train_data, DAYS)\n",
    "# train_data = remove_voice(train_data)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "tdcopy = pd.DataFrame(train_data)\n",
    "train_data[\"Label\"] = train_data[\"Label\"].map(labels)\n",
    "\n",
    "if VERBOSE:\n",
    "    print_and_log_header(\"TRAIN DATA\")\n",
    "    print_and_log(train_data.describe())\n",
    "    print_and_log(train_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Separate Labels\n",
    "train_labels = train_data.pop(target_label)\n",
    "img_paths = [\"Path{}\".format(channel) for channel in CHANNELS]\n",
    "train_data = train_data[img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "          Parsing Training Data           \n",
      "##########################################\n",
      "Start: 2018-12-12 06:26:44.799880\n",
      "End: 2018-12-12 06:26:45.458458\n",
      "Finished in 00:00:00:00.658\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Vectors of filenames.\n",
    "t_f, v_f, s_f = [], [], []\n",
    "for i in range(1, 1 + len(CHANNELS)):\n",
    "    channel = CHANNELS[i-1]\n",
    "    l = \"Path{}\".format(channel)\n",
    "    t_f.append(tf.constant(train_data[l]))\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i]\n",
    "# Vectors of labels\n",
    "train_labels = tf.constant(train_labels)\n",
    "\n",
    "# Make datasets from filenames and labels\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_labels, *t_f))\n",
    "print_and_log_header(\"Parsing Training Data\")\n",
    "train_data = timer(lambda: train_data.map(_parse_function))\n",
    "print_and_log(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 TRAINING                 \n",
      "##########################################\n",
      "<MapDataset shapes: ((1, 28, 28, 4), ()), types: (tf.float32, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print_and_log_header(\"TRAINING\")\n",
    "print_and_log(train_data)\n",
    "print_and_log(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=paths[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the input functions.\n",
    "training_eval_input_fn = create_predict_input_fn(train_data, DEFAULT_BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Prediction  Probability  Category  Day  Month  Label  SequenceNumber  \\\n",
      "0              1     0.518007  no_voice   11     12      0             430   \n",
      "1              1     0.521704  no_voice   11     12      1            2361   \n",
      "2              1     0.524322  no_voice   11     12      0            1241   \n",
      "3              1     0.519174  no_voice   12     12      0            3321   \n",
      "4              1     0.521558  no_voice   11     12      1            2964   \n",
      "5              1     0.525201  no_voice   12     12      1             974   \n",
      "6              1     0.519320  no_voice   11     12      0            3040   \n",
      "7              1     0.521548  no_voice   12     12      1            2023   \n",
      "8              1     0.524248  no_voice   12     12      0            1143   \n",
      "9              1     0.525673  no_voice   12     12      0            1092   \n",
      "10             1     0.524397  no_voice   11     12      1            2684   \n",
      "11             1     0.520771  no_voice   11     12      0             474   \n",
      "12             1     0.524213  no_voice   12     12      0            1214   \n",
      "13             1     0.527765  no_voice   12     12      1            2814   \n",
      "14             1     0.526343  no_voice   12     12      0             554   \n",
      "15             1     0.525377  no_voice   11     12      1            4280   \n",
      "16             1     0.522340  no_voice   12     12      0             621   \n",
      "17             1     0.519764  no_voice   11     12      1            2052   \n",
      "18             1     0.522053  no_voice   11     12      0            3501   \n",
      "19             1     0.523421  no_voice   11     12      1            1903   \n",
      "20             1     0.529547  no_voice   11     12      0            3210   \n",
      "21             1     0.525029  no_voice   12     12      1            2272   \n",
      "22             1     0.519857  no_voice   11     12      1            4023   \n",
      "23             1     0.524188  no_voice   12     12      0            4130   \n",
      "24             1     0.532962  no_voice   12     12      0             634   \n",
      "25             1     0.522115  no_voice   12     12      1             521   \n",
      "26             1     0.523902  no_voice   12     12      1             400   \n",
      "27             1     0.530495  no_voice   11     12      1            1104   \n",
      "28             1     0.523939  no_voice   12     12      1            1041   \n",
      "29             1     0.521042  no_voice   12     12      1             534   \n",
      "...          ...          ...       ...  ...    ...    ...             ...   \n",
      "8765           1     0.519555  no_voice   11     12      1            2251   \n",
      "8766           1     0.528424  no_voice   11     12      1            3511   \n",
      "8767           1     0.521102  no_voice   11     12      1            2350   \n",
      "8768           1     0.521129  no_voice   12     12      0            1313   \n",
      "8769           1     0.514308  no_voice   11     12      1             230   \n",
      "8770           1     0.529566  no_voice   12     12      1            1641   \n",
      "8771           1     0.535375  no_voice   11     12      1            1282   \n",
      "8772           1     0.530279  no_voice   11     12      1            1541   \n",
      "8773           1     0.518550  no_voice   12     12      1            2424   \n",
      "8774           1     0.518233  no_voice   11     12      1            3622   \n",
      "8775           1     0.521290  no_voice   12     12      0             534   \n",
      "8776           1     0.518199  no_voice   11     12      1            4174   \n",
      "8777           1     0.525570  no_voice   11     12      0            4323   \n",
      "8778           1     0.515616  no_voice   11     12      0             981   \n",
      "8779           1     0.522637  no_voice   12     12      1            1232   \n",
      "8780           1     0.521280  no_voice   12     12      0            4240   \n",
      "8781           1     0.526398  no_voice   11     12      0            2234   \n",
      "8782           1     0.526118  no_voice   11     12      0             213   \n",
      "8783           1     0.518978  no_voice   11     12      0             170   \n",
      "8784           1     0.519880  no_voice   11     12      0            3991   \n",
      "8785           1     0.522836  no_voice   11     12      0            2213   \n",
      "8786           1     0.522796  no_voice   12     12      1             771   \n",
      "8787           1     0.521064  no_voice   12     12      0            2910   \n",
      "8788           1     0.521192  no_voice   11     12      0            3133   \n",
      "8789           1     0.523451  no_voice   11     12      0            2743   \n",
      "8790           1     0.523414  no_voice   12     12      1            1371   \n",
      "8791           1     0.524760  no_voice   11     12      1            2630   \n",
      "8792           1     0.523700  no_voice   11     12      0            3014   \n",
      "8793           1     0.524587  no_voice   11     12      0            2113   \n",
      "8794           1     0.521327  no_voice   12     12      1             390   \n",
      "\n",
      "             Set                                              Path1  \\\n",
      "0       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "1       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "2       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "3       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "4        Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "5     Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "6       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "7       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "9       Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "10       Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "11    Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "12       Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "13    Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "14       Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "15      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "16      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "17      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "18      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "19      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "20      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "21      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "22      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "23      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "24    Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "25      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "26      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "27    Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "28      Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "29       Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "...          ...                                                ...   \n",
      "8765    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8766    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8767    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8768    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8769    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8770    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8771    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8772    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8773  Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8774    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8775     Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8776     Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8777    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8778    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8779    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8780    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8781  Validation  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8782    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8783    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8784    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8785    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8786    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8787    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8788    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8789    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8790    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8791    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8792     Testing  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8793    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "8794    Training  /Users/kyy/cerebro_train/images_scaled/osc_tes...   \n",
      "\n",
      "                                                  Path2   ...   Path13 Path14  \\\n",
      "0     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "1     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "2     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "3     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "4     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "5     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "6     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "7     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "9     /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "10    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "11    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "12    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "13    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "14    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "15    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "16    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "17    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "18    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "19    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "20    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "21    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "22    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "23    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "24    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "25    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "26    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "27    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "28    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "29    /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "...                                                 ...   ...      ...    ...   \n",
      "8765  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8766  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8767  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8768  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8769  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8770  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8771  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8772  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8773  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8774  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8775  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8776  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8777  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8778  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8779  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8780  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8781  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8782  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8783  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8784  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8785  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8786  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8787  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8788  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8789  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8790  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8791  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8792  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8793  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "8794  /Users/kyy/cerebro_train/images_scaled/osc_tes...   ...      NaN    NaN   \n",
      "\n",
      "      Path15  Path16  Path17  Path18  Path19  Path20  Path21  Path22  \n",
      "0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "5        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "6        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "7        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "9        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "11       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "12       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "13       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "14       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "15       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "16       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "17       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "18       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "19       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "20       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "21       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "22       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "23       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "24       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "25       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "26       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "27       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "28       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "29       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...  \n",
      "8765     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8766     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8767     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8768     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8769     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8770     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8771     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8772     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8773     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8774     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8775     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8776     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8777     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8778     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8779     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8780     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8781     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8782     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8783     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8784     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8785     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8786     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8787     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8788     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8789     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8790     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8791     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8792     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8793     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8794     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[8795 rows x 26 columns]\n",
      "##########################################\n",
      "                 OUTLIERS                 \n",
      "##########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>SequenceNumber</th>\n",
       "      <th>Path9</th>\n",
       "      <th>Path10</th>\n",
       "      <th>Path11</th>\n",
       "      <th>Path12</th>\n",
       "      <th>Path13</th>\n",
       "      <th>Path14</th>\n",
       "      <th>Path15</th>\n",
       "      <th>Path16</th>\n",
       "      <th>Path17</th>\n",
       "      <th>Path18</th>\n",
       "      <th>Path19</th>\n",
       "      <th>Path20</th>\n",
       "      <th>Path21</th>\n",
       "      <th>Path22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>879.0</td>\n",
       "      <td>879.000000</td>\n",
       "      <td>879.000000</td>\n",
       "      <td>879.0</td>\n",
       "      <td>879.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.515636</td>\n",
       "      <td>11.391354</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2320.557452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.488331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1391.198090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507471</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514684</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>833.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516080</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2560.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517087</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3450.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517816</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4394.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prediction  Probability         Day  Month  SequenceNumber  Path9  \\\n",
       "count       879.0   879.000000  879.000000  879.0      879.000000    0.0   \n",
       "mean          1.0     0.515636   11.391354   12.0     2320.557452    NaN   \n",
       "std           0.0     0.001857    0.488331    0.0     1391.198090    NaN   \n",
       "min           1.0     0.507471   11.000000   12.0        0.000000    NaN   \n",
       "25%           1.0     0.514684   11.000000   12.0      833.500000    NaN   \n",
       "50%           1.0     0.516080   11.000000   12.0     2560.000000    NaN   \n",
       "75%           1.0     0.517087   12.000000   12.0     3450.500000    NaN   \n",
       "max           1.0     0.517816   12.000000   12.0     4394.000000    NaN   \n",
       "\n",
       "       Path10  Path11  Path12  Path13  Path14  Path15  Path16  Path17  Path18  \\\n",
       "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Path19  Path20  Path21  Path22  \n",
       "count     0.0     0.0     0.0     0.0  \n",
       "mean      NaN     NaN     NaN     NaN  \n",
       "std       NaN     NaN     NaN     NaN  \n",
       "min       NaN     NaN     NaN     NaN  \n",
       "25%       NaN     NaN     NaN     NaN  \n",
       "50%       NaN     NaN     NaN     NaN  \n",
       "75%       NaN     NaN     NaN     NaN  \n",
       "max       NaN     NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "                 KEEPERS                  \n",
      "##########################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>SequenceNumber</th>\n",
       "      <th>Path9</th>\n",
       "      <th>Path10</th>\n",
       "      <th>Path11</th>\n",
       "      <th>Path12</th>\n",
       "      <th>Path13</th>\n",
       "      <th>Path14</th>\n",
       "      <th>Path15</th>\n",
       "      <th>Path16</th>\n",
       "      <th>Path17</th>\n",
       "      <th>Path18</th>\n",
       "      <th>Path19</th>\n",
       "      <th>Path20</th>\n",
       "      <th>Path21</th>\n",
       "      <th>Path22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7916.0</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>7916.0</td>\n",
       "      <td>7916.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.524195</td>\n",
       "      <td>11.511748</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2181.893633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.499894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1254.613843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517818</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.521375</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1111.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.523993</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2151.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526622</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3274.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.537190</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4394.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prediction  Probability          Day   Month  SequenceNumber  Path9  \\\n",
       "count      7916.0  7916.000000  7916.000000  7916.0     7916.000000    0.0   \n",
       "mean          1.0     0.524195    11.511748    12.0     2181.893633    NaN   \n",
       "std           0.0     0.003577     0.499894     0.0     1254.613843    NaN   \n",
       "min           1.0     0.517818    11.000000    12.0        0.000000    NaN   \n",
       "25%           1.0     0.521375    11.000000    12.0     1111.000000    NaN   \n",
       "50%           1.0     0.523993    12.000000    12.0     2151.500000    NaN   \n",
       "75%           1.0     0.526622    12.000000    12.0     3274.000000    NaN   \n",
       "max           1.0     0.537190    12.000000    12.0     4394.000000    NaN   \n",
       "\n",
       "       Path10  Path11  Path12  Path13  Path14  Path15  Path16  Path17  Path18  \\\n",
       "count     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "mean      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "min       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "25%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "50%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "75%       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "max       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Path19  Path20  Path21  Path22  \n",
       "count     0.0     0.0     0.0     0.0  \n",
       "mean      NaN     NaN     NaN     NaN  \n",
       "std       NaN     NaN     NaN     NaN  \n",
       "min       NaN     NaN     NaN     NaN  \n",
       "25%       NaN     NaN     NaN     NaN  \n",
       "50%       NaN     NaN     NaN     NaN  \n",
       "75%       NaN     NaN     NaN     NaN  \n",
       "max       NaN     NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create predicitons and remove 10% lowest confidence rows\n",
    "results = [x for x in classifier.predict(input_fn=training_eval_input_fn)]\n",
    "classes = [x[\"classes\"] for x in results]\n",
    "probs = [x[\"probabilities\"] for x in results]\n",
    "probs = pd.Series(probs)\n",
    "probs = probs.apply(lambda x: max(x))\n",
    "tdf = pd.DataFrame({\"Prediction\":classes, \"Probability\":probs})\n",
    "num_items = tdf.shape[0]\n",
    "for k in tdcopy:\n",
    "    tdf[k] = tdcopy[k]\n",
    "outliers = tdf.nsmallest(int(num_items * OUTLIER_PERCENTAGE), \"Probability\")\n",
    "keepers = tdf.append(outliers, ignore_index=True).drop_duplicates([\"Day\", \"Month\", \"Label\", \"SequenceNumber\"], keep=False).reset_index(drop=True)\n",
    "print(tdf)\n",
    "outliers = outliers.reset_index(drop=True)\n",
    "keepers[\"Label\"] = keepers[\"Label\"].apply(lambda x: reverse_lookup[x])\n",
    "outliers[\"Label\"] = outliers[\"Label\"].apply(lambda x: reverse_lookup[x])\n",
    "if DISPLAY:\n",
    "    print_and_log_header(\"OUTLIERS\")\n",
    "    display.display(outliers.describe())\n",
    "    print_and_log_header(\"KEEPERS\")\n",
    "    display.display(keepers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "keepers.drop([\"Prediction\", \"Probability\"], axis=1, inplace=True)\n",
    "outliers.drop([\"Prediction\", \"Probability\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIERS\n",
      "\tTraining:\t694\n",
      "\tValidation:\t83\n",
      "\tTesting:\t102\n"
     ]
    }
   ],
   "source": [
    "s = outliers[\"Set\"]\n",
    "tra = sum(s.apply(lambda x: 1 if x == \"Training\" else 0))\n",
    "val = sum(s.apply(lambda x: 1 if x == \"Validation\" else 0))\n",
    "tst = sum(s.apply(lambda x: 1 if x == \"Testing\" else 0))\n",
    "print(\"OUTLIERS\\n\\tTraining:\\t{}\\n\\tValidation:\\t{}\\n\\tTesting:\\t{}\".format(tra, val, tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KEEPERS to 'keepers_12_11_1234.csv'\n",
      "Saved OUTLIERS to 'outliers_12_11_1234.csv'\n"
     ]
    }
   ],
   "source": [
    "KEEPERS_NAME = \"keepers_{:02}_{:02}_{}.csv\".format(MONTHS[0], DAYS[0], NUMS)\n",
    "OUTLIERS_NAME = \"outliers_{:02}_{:02}_{}.csv\".format(MONTHS[0], DAYS[0], NUMS)\n",
    "keepers.to_csv(KEEPERS_NAME)\n",
    "print(\"Saved KEEPERS to '{}'\".format(KEEPERS_NAME))\n",
    "outliers.to_csv(OUTLIERS_NAME)\n",
    "print(\"Saved OUTLIERS to '{}'\".format(OUTLIERS_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
