{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "\n",
    "\n",
    "# global variables\n",
    "TFR_TRAIN = 'train.tfrecord'\n",
    "TFR_VALID = 'valid.tfrecord'\n",
    "TFR_TEST = 'test.tfrecord'\n",
    "BUCKET = 'gs://robolab/'\n",
    "\n",
    "# image and classes\n",
    "NUM_CLASSES = 2\n",
    "IMG_HEIGHT = 80\n",
    "IMG_WIDTH = 71\n",
    "\n",
    "# model dir\n",
    "OUTDIR = BUCKET + 'output_TPU'\n",
    "\n",
    "# hypers\n",
    "BATCH_SIZE = 800\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = 10\n",
    "LR = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "# TPU config\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "    None,\n",
    "    zone=None,\n",
    "    project=None)\n",
    "\n",
    "tpu_config = tf.contrib.tpu.TPUConfig(\n",
    "    iterations_per_loop=2,\n",
    "    num_shards=8,\n",
    "    per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2)\n",
    "    \n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTDIR,\n",
    "    tpu_config=tpu_config)\n",
    "\n",
    "\n",
    "\n",
    "def parser(serialized_example):\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float32)\n",
    "    image.set_shape([IMG_HEIGHT * IMG_WIDTH])\n",
    "    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = tf.expand_dims(image, axis=2)\n",
    "    \n",
    "    label = tf.decode_raw(features['label'], tf.int32)\n",
    "    label.set_shape([1])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def test_parser(serialized_example):\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                 'image_id': tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float32)\n",
    "    image.set_shape([IMG_HEIGHT * IMG_WIDTH])\n",
    "    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = tf.expand_dims(image, axis=2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def train_input_fn(params):\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_TRAIN)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(1024)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            parser,\n",
    "            batch_size=batch_size,\n",
    "            num_parallel_batches=8,\n",
    "            drop_remainder=True))\n",
    "\n",
    "    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(params):\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_TRAIN)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            parser,\n",
    "            batch_size=batch_size,\n",
    "            num_parallel_batches=8,\n",
    "            drop_remainder=True))\n",
    "    \n",
    "    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def valid_input_fn(params):\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_VALID)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.repeat(1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def predict_input_fn():\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_TEST)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.map(test_parser)\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features = iterator.get_next()\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_image_id(serialized_example):\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                  'image_id': tf.FixedLenFeature([], tf.string)})\n",
    "    \n",
    "    return features['image_id']\n",
    "\n",
    "\n",
    "\n",
    "def metrics_fn(classes, labels):\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        labels=labels,\n",
    "        predictions=classes)\n",
    "    \n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):\n",
    "\n",
    "    conv_layer_1 = tf.layers.conv2d(\n",
    "        inputs=features,\n",
    "        filters=8,\n",
    "        kernel_size=[2, 2],\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool_layer_1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv_layer_1,\n",
    "        pool_size=[2, 2],\n",
    "        strides=2,\n",
    "        padding='same')\n",
    "\n",
    "    conv_layer_2 = tf.layers.conv2d(\n",
    "        inputs=pool_layer_1,\n",
    "        filters=32,\n",
    "        kernel_size=[2, 2],\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool_layer_2 = tf.layers.max_pooling2d(\n",
    "        inputs=conv_layer_2,\n",
    "        pool_size=[2, 2],\n",
    "        strides=2,\n",
    "        padding='same')\n",
    "\n",
    "    reshape_layer = tf.layers.flatten(pool_layer_2)\n",
    "\n",
    "    dense_layer = tf.layers.dense(\n",
    "        inputs=reshape_layer,\n",
    "        units=256,\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    is_train = False\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        is_train = True\n",
    "\n",
    "    dropout_layer = tf.layers.dropout(\n",
    "        inputs=dense_layer,\n",
    "        rate=0.2,\n",
    "        training=is_train)\n",
    "\n",
    "    logits = tf.layers.dense(\n",
    "        inputs=dropout_layer,\n",
    "        units=NUM_CLASSES)\n",
    "    \n",
    "    classes = tf.argmax(logits, axis=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return estimator.EstimatorSpec(mode=mode,\n",
    "                                       predictions={'classes':classes,\n",
    "                                                    'probabilities':tf.nn.softmax(logits, axis=1)})\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=labels,\n",
    "        logits=logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                                       \n",
    "        train_optimizer = tf.train.AdamOptimizer(learning_rate=LR)\n",
    "        train_optimizer = tf.contrib.tpu.CrossShardOptimizer(train_optimizer).minimize(loss=loss,\n",
    "                                                                            global_step=tf.train.get_global_step())\n",
    "        \n",
    "        return tf.contrib.tpu.TPUEstimatorSpec(mode=mode,\n",
    "                                               loss=loss,\n",
    "                                               train_op=train_optimizer)\n",
    "    \n",
    "    # EVAL mode\n",
    "    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode,\n",
    "                                           loss=loss,\n",
    "                                           eval_metrics=(metrics_fn, [labels, classes]))\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(estimator):\n",
    "\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
    "    estimator.evaluate(input_fn=eval_input_fn, steps=EVAL_STEPS)\n",
    "\n",
    "\n",
    "\n",
    "cnn_classifier = tf.contrib.tpu.TPUEstimator(\n",
    "    model_fn=cnn_model_fn,\n",
    "    config=run_config,\n",
    "    use_tpu=True,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    eval_on_tpu=True)\n",
    "\n",
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
